---
layout: post
title:  "The expectation-maximization algorithm - Part 1"
date:   2021-07-03
categories: [mathematics]
tags: ['machine learning', 'mathematics', 'Mathematica', 'optimization']
description: An introduction to the expectation-maximization algorithm focusing on the concept of maximum likelihood estimation
---

### Contents
{:.no_toc}

The expectation-maximization (EM) algorithm is an iterative method to find the local maximum likelihood of parameters in statistical models. So what is the maximum likelihood? It's the maximum value of the likelihood function! And what is a likelihood function? It's a function of the parameters treating the observed data as fixed points, i.e., we write $$L(θ|x)$$. If $$L(θ_1|x) > L(θ_2|x)$$ then the sample we actually observed is more likely to have occurred if $$θ = θ_1$$ rather than if $$θ = θ_2$$. So, given the data that we have observed, the likelihood function points us to those more plausible parameters that have generated the observed data.

The EM algorithm is particularly useful when there are missing data in the data set or when the model depends on hidden or so-called latent variables. There are variables that affect our observed data but in ways that we can't know. EM takes its name from the alternation between two algorithmic steps. The first step is the expectation step, where we develop a function for the expectation of the log-likelihood, using the current best estimates of the model's parameters. On the other hand, the maximization step calculates the parameters' values that maximize the expected log-likelihood. These new estimates of the parameters are then used to determine the distribution of the latent variables in the next expectation step. 

Let us consider some observed 1-dimensional data points, $$x_i$$. We assume they are generated by 2 normal distributions $$N(μ_1, σ_1^2)$$ and $$N(μ_1, σ_2^2)$$, with probabilities $$p$$ and $$1-p$$, respectively. In this setup, we have 5 unknown parameters, the mixing probability p, the mean and standard deviation of the first distribution, and the mean and standard deviation of the second distribution. Let us gather all these under a vector called $$\theta = [\pi, \mu_1, \sigma_1, \mu_2, \sigma_2]$$.

Suppose that we observed a datapoint with value $$x_i$$. What is the probability of $$x_i$$ occuring? Assuming $$\varphi_1(x)$$ is the probability distribution function of the 1st distribution, and $$\varphi_2(x)$$ of the second, the probability of observing $$x_i$$ is:

$$p(x_i) = \pi \varphi_1(x_i) + (1-p)\varphi_2(x_i)$$

To be more pedantic we would write:

$$p(x_i|\theta) = \pi \varphi_1(x_i|\mu_1,\sigma_1^2) + (1-p)\varphi_2(x_i|\mu_2,\sigma_2^2)$$

Which means that the PDF's are paremeterized by $$\mu_1,\sigma_1^2$$ and $$\mu_2, \sigma_2^2$$, respectively. Ok, but this is just for a single observation $$x_i$$. What if we have a bunch of $$x_i$$'s, say for $$i=1,\ldots,N$$? To find the joint probability of $$N$$ independent events (which by the way is the likelihood function!) we just multiply the individual probabilities:

$$L(\theta|x) = \prod_{i=1}^N p(x_i|\theta)$$

But since it's easier to work with sums rather than products, we take the logarirthm of the likelihood, $$\ell(\theta|x)$$:

$$\begin{align*}\ell(\theta|x) &= \log \prod_{i=1}^N p(x_i|\theta) =\sum_{i=1}^N \log p(x_i|\theta)\\&=\sum_{i=1}^N \log \left[\pi \varphi_1(x_i|\mu_1,\sigma_1^2) + (1-p)\varphi_2(x_i|\mu_2,\sigma_2^2)\right]\end{align*}$$

So, our objective is to maximize likelihood $$L(\theta|x)$$, which is equivalent to maximizing the log-likelihood $$\ell(\theta|x)$$, with respect to the model's parameters $$\theta = [\pi, \mu_1, \sigma_1, \mu_2, \sigma_2]$$, *given* the data points $$\{x_i\}$$. 

In the following examples, we will generate some synthetic observed data from a mixture distribution with known parameters and mixing probability $$p$$. We will then calculate $$\ell(\theta|x)$$ for various parameter values by keeping the rest fixed. Every time we will do that, we will see how $$\ell(\theta|x)$$ is maximized when the model's parameter that we vary becomes equal to the ground-truth value.

Let's create a mixture distribution of two Gaussian distributions with known parameters $$m_1, s_1, m_2, s_2$$ and known mixing probability $$p=0.3$$. Normally, we won't know the values of these parameters, and as a matter of fact, finding them will be the very objective of the EM algorithm. But for now, let's *pretend* we don't know them.

{% highlight mathematica %}
{% raw %}
ClearAll["Global`*"];
{m1, s1} = {1, 2};
{m2, s2} = {9, 3};

npts = 5000;
dist[m_, s_] := NormalDistribution[m, s];
mixdist[p_] :=
 MixtureDistribution[{p, 1 - p}, {dist[m1, s1], dist[m2, s2]}]
data = RandomVariate[mixdist[0.3], npts];
Histogram[data]
{% endraw %}
{% endhighlight %}

<p align="center">
 <img style="width: 100%; height: 100%" src="{{ site.url }}/images/em_algorithm/histogram.png" alt="Histogram of mixture distribution">
</p>

Let's plot the probability density functions of the mixture distribution for various mixing probabilities $$p$$. We notice how for $$p\to 0$$ the mixture distribution approaches the 1st distribution, and for $$p\to 1$$, the 2nd distribution. For in-between values, it's a mixture! ;)

{% highlight mathematica %}
{% raw %}
Style[Grid[{
   Table[
    Plot[PDF[mixdist[p], x], {x, -10, 20}, 
     PlotLabel -> "p=" <> ToString@p,
     FrameLabel -> {"x", "PDF(x)"}, 
     Frame -> {True, True, False, False},
     AxesOrigin -> {-10, 0}, Filling -> Axis],
    {p, 0, 1, 0.3}]
   }],
 ImageSizeMultipliers -> 0.7]
{% endraw %}
{% endhighlight %}

<p align="center">
 <img style="width: 100%; height: 100%" src="{{ site.url }}/images/em_algorithm/varying_mixing_prob.png" alt="PDF of mixture distribution for varying mixing probability">
</p>

Let us now define the log-likelihood function:

{% highlight mathematica %}
{% raw %}
logLikelihood[data_, p_, m1_, s1_, m2_, s2_] :=
 Module[{},
  Sum[
   Log[
    p PDF[dist[m1, s1], x] + (1 - p) PDF[dist[m2, s2], x] /. 
     x -> data[[i]]
    ],
   {i, 1, Length@data}]
  ]
  {% endraw %}
{% endhighlight %}

Ok, we are ready to go. We will first vary the mixing probability $$\pi$$, keeping the rest of the model's parameters fixed. In some sense, we are brute-forcing $$\pi$$, to find $$\pi_

{% highlight mathematica %}
{% raw %}
llvalues = 
  Table[{p, logLikelihood[yt, p, m1, s1, m2, s2]}, {p, 0, 1, 0.1}];
{pmax, llmax} = 
 llvalues[[Ordering[llvalues[[All, 2]], -1][[1]]]]
(* {0.3, -14437.1} *)

plot1 =
 Show[
  ListPlot[llvalues, Joined -> True, 
   FrameLabel -> {"Probability p", "Log-Likelihood"}, 
   Frame -> {True, True, False, False}, 
   GridLines -> {{pmax}, {llmax}}, GridLinesStyle -> Dashed],
  ListPlot[llvalues, PlotStyle -> {Red, AbsolutePointSize[5]}]
  ]
{% endraw %}
{% endhighlight %}

<p align="center">
 <img style="width: 100%; height: 100%" src="{{ site.url }}/images/em_algorithm/log_likelihood_p.png" alt="Log likelihood for varying mixing probability">
</p>

Do you see how $$\ell(\theta|x)$$ is maximized at $$\pi = 0.3$$? By the same token, we can try other model parameters, but we always come to the same conclusion: the log-likelihood, therefore the likelihood, is maximized when our guesses become equal to the ground-truth values for the model's parameters.

<p align="center">
 <img style="width: 100%; height: 100%" src="{{ site.url }}/images/em_algorithm/log_likelihood_combined.png" alt="Log likelihood for varying mixing probability, mean and standard deviation">
</p>
