---
layout: post
title:  What does it really mean for a machine to learn mathematics?
date:   2019-09-27
categories: math
tags: ['machine learning', math, 'neural netowrks']
---

### Introduction
This post was triggered by a paper [posted here](https://openreview.net/pdf?id=S1eZYeHFDS), titled "Deep learning from symbolic mathematics". The authors trained a sequence-to-sequence neural network that was able to perform symbolic integration and solve differential equations. Remarkably, they achieved results that outperform commercial Computer Algebra Systems (CAS) such as [Matlab](https://www.mathworks.com/products/matlab.html) and [Mathematica](https://www.wolfram.com/mathematica/). It's interesting to see how they built their training set.

### Training sets
#### Integration
The training set for integration was generated by starting with random functions (with a certain upper bound in terms of their length) and calculating their derivative with a CAS. Then the derivatives becomes the input target and the original functions the output. For instance, suppose that we start with the function:

$$
f(x) = \ln(\ln(x)) \Rightarrow f'(x) = \frac{1}{\ln(x)} \cdot (\ln(x))' = \frac{1}{x \ln(x)}
$$

Then, in our training set we consider $$\frac{1}{x \ln(x)}$$ as the input, i.e. the function that we want to integrate, and $$\ln(\ln(x))$$ becomes the desired output of our model.

$$
\int \frac{1}{x \ln(x)} = \ln(\ln(x)) + C
$$

#### Solving first order differential equations
The training set for solving first order differential equations is generated in a similar way as previously, i.e. with a "reverse logic". Let's start with a random function of two variables $$F(x,y) = c$$, where $$c$$ is constant. Then if $$F$$ is chosen such that it is "solvable for $$y$$", meaning that we can write $$y = f(x, c) = f_c(x)$$, the function $$F$$ can be expressed as $$F(x, f_c(x)) = c$$. If we differentiate with respect to $$x$$, we get:

$$
\frac{\partial F(x, f_c(x))}{\partial x} + \frac{\partial F(x,f_c(x))}{\partial f_c(x)} \frac{\partial f_c(x)}{\partial x} = 0
$$

Recall that when we want to calculate the *total derivative* of a function $$y = f(t, u_1, u_2, \ldots)$$, where the intermediate variables $$u_k$$ are functions of the form $$u_k = u_k(t, u_1, u_2, \ldots)$$, then $$\frac{\partial y}{\partial t} = \frac{\partial f}{\partial t} + \frac{\partial f}{\partial u_1}\frac{\partial u_1}{\partial t} + \ldots$$, since $$y$$'s value are affected directly by $$t$$ but also indirectly via the intermediate variables $$u_k$$ that are functions of $$t$$ as well. If we let $$y = f_c(x)$$, then:

$$
\frac{\partial F(x, y)}{\partial x} + \frac{\partial F(x, y)}{\partial y} \frac{\partial y}{\partial x} = 0
$$

Therefore, first we "found" the solution $$y = f_c(x)$$ and then we constructed the differential equation that $$y$$ solves, i.e.

$$
\frac{\partial F(x, y)}{\partial x} + \frac{\partial F(x, y)}{\partial y} y' = 0
$$

It feels like cheating, huh? A really simple example, where we consider $$F(x,y) = x \ln{y} = c$$:

$$
\underbrace{x \ln{y}}_{F(x,y)} = c \Rightarrow \ln{y} = c/x \Rightarrow y = e^{c/x}
$$

Then by differentiating with respect to $$x$$ we wget:

$$
\ln{y} + \frac{x}{y}y' = 0 \Rightarrow  x y' + y\ln{y} = 0
$$

You might want to verify that $$y = e^{c/x}$$ is a solution of $$x y' + y\ln{y} = 0$$. So $$y = e^{c/x}$$ becomes the target of the neural network and $$x y' + y\ln{y} = 0$$ the input, i.e. the differential equation that we'd like to solve.

### Represeting mathematical expressions as sequences
Mathematical expressions are modelled as trees with operators as internal nodes, and numbers, constants or variables, as leaves. By enumerating nodes in prefix order, the authors transformed trees into *sequences* suitable for natural language processing models. For example the expression $$2 + 3 + 5$$ is represented by the sequence "+ 2 + 3 5".

<p align="center">
 <img style="width: 25%; height: 25%" src="{{ site.url }}/images/math_tree.png">
</p>


