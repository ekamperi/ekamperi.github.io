---
layout: post
title:  What does it really mean for a machine to learn mathematics?
date:   2019-09-27
categories: math
tags: ['machine learning', math, 'neural netowrks']
---

### Introduction
This post was triggered by a paper [posted here](https://openreview.net/pdf?id=S1eZYeHFDS), titled "Deep learning from symbolic mathematics". The authors trained a sequence-to-sequence neural network that was able to perform symbolic integration and solve differential equations. Remarkably, they achieved results that outperform commercial Computer Algebra Systems (CAS) such as [Matlab](https://www.mathworks.com/products/matlab.html) and [Mathematica](https://www.wolfram.com/mathematica/).

### Integration
The training set for integration was generated by starting with random functions (with a certain upper bound in terms of their length) and calculating their derivative with a CAS. Then the derivative becomes the input target and the original function the output. For instance, suppose that we start with the function:

$$
f(x) = \ln(\ln(x)) \Rightarrow f'(x) = \frac{1}{\ln(x)} \cdot (\ln(x))' = \frac{1}{x \ln(x)}
$$

Then, in our training set we consider $$\frac{1}{x \ln(x)}$$ as the input, i.e. the function that we want to integrate, and $$\ln(\ln(x))$$ becomes the desired output of our model.

$$
\int \frac{1}{x \ln(x)} = \ln(\ln(x)) + C
$$

### Solving first order differential equations
The training set for solving first order differential equations is generatoed in a similar way as previously, i.e. with a "reverse logic". Let's start with a random function of two variables $$F(x,y) = c$$, $$c$$ constant. Then if $$F$$ is such that is "solvable for $$y$$", meaning that we can write $$y = f(x, c)$$, the function can be written as $$F(x, f_c(x)) = c$$. 
