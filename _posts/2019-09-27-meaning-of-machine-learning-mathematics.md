---
layout: post
title:  What does it really mean for a machine to learn mathematics?
date:   2019-09-27
categories: math
tags: ['machine learning', math, 'neural netowrks']
---

### Contents
{:.no_toc}

* A markdown unordered list which will be replaced with the ToC, excluding the "Contents header" from above
{:toc}

### Introduction
This post was triggered by a paper [posted here](https://openreview.net/pdf?id=S1eZYeHFDS), titled "Deep learning from symbolic mathematics". The authors trained a sequence-to-sequence neural network that was able to perform symbolic integration and solve differential equations. Remarkably, they achieved results that outperform commercial Computer Algebra Systems (CAS) such as [Matlab](https://www.mathworks.com/products/matlab.html) and [Mathematica](https://www.wolfram.com/mathematica/). It's interesting to see how they built their training set.

### Training sets
#### Integration
The training set for integration was generated by starting with random functions (with a certain upper bound in terms of their length) and calculating their derivative with a CAS. Then the derivatives become the input targets and the original functions the output. For instance, suppose that we start with the function:

$$
f(x) = \ln(\ln(x)) \Rightarrow f'(x) = \frac{1}{\ln(x)} \cdot (\ln(x))' = \frac{1}{x \ln(x)}
$$

Then, in our training set we consider $$\frac{1}{x \ln(x)}$$ as the input, i.e. the function that we want to integrate, and $$\ln(\ln(x))$$ becomes the desired output of our model.

$$
\int \frac{1}{x \ln(x)} = \ln(\ln(x)) + C
$$

#### Solving first order differential equations
The training set for solving first order differential equations is generated in a similar way as previously, i.e. with a "reverse logic". Let's start with a random function of two variables $$F(x,y) = c$$, where $$c$$ is constant. Then if $$F$$ is chosen such that it is "solvable for $$y$$", meaning that we can write $$y = f(x, c) = f_c(x)$$, the function $$F$$ can be expressed as $$F(x, f_c(x)) = c$$. If we differentiate with respect to $$x$$, we get:

$$
\frac{\partial F(x, f_c(x))}{\partial x} + \frac{\partial F(x,f_c(x))}{\partial f_c(x)} \frac{\partial f_c(x)}{\partial x} = 0
$$

Recall that when we want to calculate the *total derivative* of a function $$y = f(t, u_1, u_2, \ldots)$$, where the intermediate variables $$u_k$$ are functions of the form $$u_k = u_k(t, u_1, u_2, \ldots)$$, then $$\frac{\partial y}{\partial t} = \frac{\partial f}{\partial t} + \frac{\partial f}{\partial u_1}\frac{\partial u_1}{\partial t} + \ldots$$, since $$y$$'s value are affected directly by $$t$$ but also indirectly via the intermediate variables $$u_k$$ that are functions of $$t$$ as well. If we let $$y = f_c(x)$$, then:

$$
\frac{\partial F(x, y)}{\partial x} + \frac{\partial F(x, y)}{\partial y} \frac{\partial y}{\partial x} = 0
$$

Therefore, first we "found" the solution $$y = f_c(x)$$ and then we constructed the differential equation that $$y$$ solves, i.e.

$$
\frac{\partial F(x, y)}{\partial x} + \frac{\partial F(x, y)}{\partial y} y' = 0
$$

It feels like cheating, huh? A really simple example, where we consider $$F(x,y) = x \ln{y} = c$$:

$$
\underbrace{x \ln{y}}_{F(x,y)} = c \Rightarrow \ln{y} = c/x \Rightarrow y = e^{c/x}
$$

Then by differentiating with respect to $$x$$ we wget:

$$
\ln{y} + \frac{x}{y}y' = 0 \Rightarrow  x y' + y\ln{y} = 0
$$

You might want to verify that $$y = e^{c/x}$$ is a solution of $$x y' + y\ln{y} = 0$$. So $$y = e^{c/x}$$ becomes the target of the neural network and $$x y' + y\ln{y} = 0$$ the input, i.e. the differential equation that we'd like to solve.

### Representing mathematical expressions as sequences
Mathematical expressions are modelled as trees with operators as internal nodes, and numbers, constants or variables, as leaves. By enumerating nodes in prefix order, the authors transformed trees into *sequences* suitable for natural language processing models. For example the expression $$2 + 3 + 5$$ is represented by the sequence "+ 2 + 3 5".

<p align="center">
 <img style="width: 25%; height: 25%" src="{{ site.url }}/images/math_tree.png">
</p>

From here, it's standard theory of sequence to sequence models. To sum up, we generate pairs of derivatives - antiderivatives and pairs of differential equations - solutions, then we convert these expressions to trees and then to sequences. We train the neural network to accept such sequences and output other sequences, where hopefully the output is the correct solution to the input.

### Thoughts
#### Is pattern matching the same as "mathematical understanding"?

Suppose I am with someone that doesn't know anything about mathematics and provide him with a list of matching rules like:

$$
\begin{align*}
x^2 &\longrightarrow 2 x^1\\
x^3 &\longrightarrow 3 x^2\\
x^4 &\longrightarrow 4 x^3\\
x^5 &\longrightarrow 5 x^4\\
x^6 &\longrightarrow 6 x^5\\
x^7 &\longrightarrow 7 x^5\\
\ldots
\end{align*}
$$

After looking at these symbols, he might figure that the expression $$x^{\text{whatever}}$$ is matched with $$\text{whatever} \cdot x ^{\text{whatever} - 1}$$. And, when asked to derive $$x^{100}$$ he, correctly, writes $$100 x^{99}$$. Does this mean that he "really knows" how to differentiate the expression $$x^n$$? Also, since he doesn't know anything about the notion of limits and derivatives, he will most likely miss the meaning of "derivative as rate of change".

So, when we train a neural network to integrate by providing it with thousands examples of "derivative - antiderivative" pairs, does it really learn to integrate? And what does "really" mean in this context?

#### The utilitarian approach

The utilitarian approach states that as long as the neural network model outputs correct answers, does it really matter whether this constitutes a deeper understanding of maths or not? Even if it is merely a pattern matching ability acquired and perfected through the presentation of hundreds of thousands of examples, who cares?
