[["abstract.html", "Complexity analysis of Volumetric Modulated Arc Therapy prostate plans 1 Abstract", " Complexity analysis of Volumetric Modulated Arc Therapy prostate plans 1 Abstract Background: Volumetric Modulated Arc Therapy (VMAT) is the state of the art treatment delivery method for prostate cancer patients, enabling the dose to conform tightly around the target volume and spare the surrounding healthy tissues. However, VMAT plans are inherently complex, affecting treatment deliverability, radiation-induced carcinogenesis, and mechanical strain on the linear accelerator. Moreover, they require laborious quality assurance procedures to ensure the safety of the patients. By determining which plan features are related to complexity, we may gain insights into how to regulate the treatment planning process to keep complexity under control. Objective: To compare multiple complexity metrics described in the literature and investigate which VMAT plans characteristics relate to complexity in prostate cancer patients undergoing radical external beam radiation treatment. Methods: We calculated circumference over area, edge metric, equivalent square field, leaf travel, leaf travel modulation complexity score for VMAT (LTMCSV), mean-field area, modulation complexity score (standard and VMAT variant), plan irregularity, and short aperture score. These were studied with principal component (PCA) and mutual information analysis (MIA). LTMCSV was selected for subsequent regression analysis. The plan-related variables that were evaluated as potential predictors of complexity included the total number of monitor units, number of arcs, radiation field size, physician, dosimetrist, treatment planning system (TPS) version, conformation number (CN), and high-dose planning treatment volume (PTV). Also, the dose-volume constraints, bladder \\(V_{65}\\), \\(V_{70}\\), rectum \\(V_{50}\\), \\(V_{60}\\), and \\(V_{70}\\) were considered. Linear and logistic regression analyses were performed by treating LTMCSV as a continuous and dichotomous variable, respectively. In the former case, model selection was done by univariate and multivariate analysis, forward and backward selection. In the latter case, feature selection was implemented via LASSO regularization and k-fold cross-validation, whereas the final evaluation was performed with ROC analysis and confusion matrices. Results: A total of 217 VMAT prostate plans were exported from the oncology information system of Papageorgiou General Hospital radiation oncology department. For every plan, the complexities of all the studied indices were calculated. PCA analysis revealed that complexity metrics are correlated, forming three distinct clusters, each subsuming a different aspect of a plans complexity. Three principal components were able to explain 96.2% of the variance. Likewise, MIA confirmed the complexity metrics inherent interdependence, although there were complexity pairs with minimal MI. In univariate analysis, the number of arcs, field size, dosimetrist, TPS version, high-dose PTV, bladder \\(V_{65}\\), and rectum \\(V_{50}\\) were statistically significant determinants of complexity. In multivariate analysis, the number of arcs (p&lt;0.001), increasing field size (p&lt;0.001), certain physicians (p=0.041 and p=0.027), recent TPS version (p&lt;0.001), high-dose PTV (p=0.02), high CN (p=0.036), rectums \\(V_{50}\\) (p&lt;0.001), and \\(V_{60}\\) (p=0.001) retained their statistical significance. In logistic regression, LASSO regularization invoked sparsity by driving some of the variables coefficients to zero. The number of arcs, field size, physician, dosimetrist, TPS version, CN, high-dose PTV, bladder \\(V_{70}\\), and rectum \\(V_{70}\\) had non-zero coefficients. Conclusions: There is an abundance of complexity indices for IMRT/VMAT in the literature. Both PCA and MIA analyses suggest the existence of substantial overlap among the metrics. However, this overlap is not entirely reducible through dimensionality reduction techniques, implying that there also exists some complementarity. Prediction of VMAT plan complexity and classification of plans into high- or low- complexity by clinical and dosimetric features is feasible through both linear and logistic regression analyses. "],["introduction.html", "2 Introduction 2.1 Prostate Cancer 2.2 Volumetric Modulated Arc Therapy 2.3 Inverse treatment planning 2.4 Plan complexity", " 2 Introduction 2.1 Prostate Cancer Prostate cancer (PCa) is the most common cancer among men, second only to skin cancer. This year, an estimated 191.930 men in the United States will be diagnosed with prostate cancer, accounting for 10.6% of all new cancer cases.1 Radiation therapy (RT), along with surgery and hormonal treatment, is one of the central pillars in managing PCa. During the last years, radiotherapy technology has seen dramatic progression that allowed dose escalation while keeping normal tissue toxicity from bladder and rectum low.2,3 Nowadays, Intensity Modulated Radiation Therapy (IMRT) and its evolution, Volumetric Modulated Arc Therapy (VMAT), represent the state-of-the-art treatment techniques.4 During VMAT radiotherapy, the linear accelerator rotates around the patient and irradiates from different angles with beamlets of varying aperture shape and intensity (Fig. 2.1). The fields shape is modulated by a multileaf collimator (MLC) system consisting of a set of pairs of leaves, which can move in and out of the beams field. This beam modulation results in a dose gradient that conforms tightly to the target volume, and it drops fast inside the surrounding normal tissues. Had it not been for the modern imaging techniques, such as cone-beam computed tomography, that localize the prostate and guide the precise delivery of radiation, this highly conformal treatment would not have been possible.5,6 Figure 2.1: Left: A modern linear accelerator able to deliver volumetric modulated arc therapy. Right: A Multi Leaf Collimator (MLC) system. The leaves can move in and out of the beams field, shaping the delivered dose. 2.2 Volumetric Modulated Arc Therapy VMAT, initially introduced under the name IMAT (Intensity Modulated Arc Therapy), was first proposed in 1995,7 although another type of IMRT using rotational fan beams was submitted two years earlier by Mackie et al. under the name tomotherapy.8 In VMAT, the radiation is delivered over a continuous arc, similar to tomotherapy, but uses rotational cone beams shaped by an MLC system. The idea was to convert the intensity patterns into leaf segments delivered by a sequence of one or more arcs. In principle, one arc would suffice since it may incorporate hundreds of aperture shapes. However, the linear accelerators mechanical constraints necessitate multiple arcs to provide additional degrees of freedom to the system. Nowadays, linear accelerators can vary both the dose rate and the gantry speed at each beam angle, besides the aperture shape. These additional modulation degrees allow the creation of sophisticated dose distributions by redistributing the normal tissue dose to less critical regions. Also, partial and non-coplanar arcs can be combined depending on the clinical scenario. 2.3 Inverse treatment planning IMRT and VMAT treatment planning are inverse compared to conventional 3D conformal radiotherapy workflow. The planning treatment volumes are delineated, along with organs at risks (OARs), prescription and normal tissue constraints are set, and then the optimization algorithm seeks to find the optimal beam delivery. In its simplest form,9 the cost function that is optimized is the least square differences between the calculated dose \\(d_c(\\mathbf{r})\\) and the prescribed dose \\(d_p(\\mathbf{r})\\) of structure \\(i\\), over the 3D dose voxel matrix, weighted by some importance factor \\(\\lambda_i\\): \\[ J \\equiv J(\\mathbf{w}) = \\sum_{i=1}^N \\lambda_i \\left( d_c(\\mathbf{r}) - d_p(\\mathbf{r})\\right)_i^2 \\] There is an abundance of optimization algorithms to minimize the cost function \\(J\\), such as gradient descent and simulated annealing. In gradient descent, the weights are updated in the direction of the negative gradient of the \\(J(\\mathbf{w})\\), i.e., \\(\\mathbf{w}_{n+1} = \\mathbf{w}_n - \\alpha \\nabla_{\\mathbf{w}}{J}, \\alpha&gt;0\\). The \\(\\alpha\\) value is the learning rate. The algorithm is iterative and stops when convergence is achieved, i.e., when the gradient \\(\\nabla_{\\mathbf{w}}J(\\mathbf{w})\\) is so small that \\(\\mathbf{w}\\) does not change much. In simulated annealing, a parameter (e.g., leaf position, aperture weight) is picked randomly, and its value is changed by some random amount. The new cost function is calculated, and if it is lower \\((\\Delta J &lt; 0)\\), the change is accepted. If the new cost function is higher \\((\\Delta J &gt; 0)\\), then the change is accepted with a probability \\(p = \\text{min}\\left[1,\\exp(-\\Delta J / T)\\right]\\), via the criterion \\(p \\ge u\\in[0,1]\\), where \\(T\\) is the annealing temperature that is initialized to some high value and then slowly lowered during the optimization process, and \\(u\\) a uniform random number \\(u\\in[0,1]\\). The logic behind simulated annealing is that during the initial phase of the optimization process, we want to explore the search space by allowing some suboptimal changes to take place. However, as simulation time passes by, the temperature \\(T\\) is lowered, which lowers the probability of accepting such solutions, and the algorithm is driven to convergence. In Fig. 2.2 and 2.3 we show two examples, of gradient descent and simulated annealing, minimizing the simple one-dimensional test function from Gramacy and Lee:10 \\[ f(x) = \\frac{\\sin(10\\pi x)}{2x} + (x-1)^4, \\hspace{5mm} x \\in [0.5, 2.5] \\] Figure 2.2: Gradient descent is trapped in a local minimum. In order to overcome local minima, there exist versions of gradient descent that use momentum or adaptive learning rate. Figure 2.3: Simulated annealing with a cooling schedule \\(T(t) = 0.99^t T_0\\). During the initial phase of high temperature, the algorithm accepts suboptimal changes to explore the search space. As the temperature drops, the cost function converges to a minimum. Another versatile method of describing the goodness of treatment of a plan is to use the so-called costlets, in which case the overall cost function is given by \\(J = F(c_1,c_2,\\ldots,c_K)\\). The \\(c_i, i=1,\\ldots,K\\) are the costlets and the function \\(F\\) describes how the various costlets are combined to yield the overall optimization target.11 A natural way to merge the \\(c_i\\) into a penalty function is to calculate their weighted sum, e.g. \\(J=\\sum_{i=1}^K \\lambda_i c_i\\). In this framework, a plans complexity could be encoded in a complexity costlet, as per the following simplified example: \\[\\begin{align*} J &amp;= 0.2\\underbrace{\\sum_{i\\in SC} \\text{max}(d_i - 45, 0)^{10}}_\\text{No more than 45 in Spinal Cord} + 0.3\\underbrace{\\sum_{i\\in PTV} \\text{max}(d_i - 63, 57 - d_i, 0)^2}_\\text{No less than 57 and no more than 63 in PTV} \\\\ &amp;+ 0.1\\underbrace{\\sum_{i\\in NT} \\text{max}(d_i, 0)^2}_\\text{Minimize dose to Normal Tissues} + 0.05 \\times \\text{estimated time of delivery}\\\\ &amp;+ 0.05 \\times \\text{tongue &amp; groove effect} + 0.05 \\times \\text{complexity} + \\ldots \\end{align*}\\] Once inverse optimization derives an optimal fluence per field, the process is handed over to the leaf sequencing algorithm that determines a sequence of leaf motions to deliver that fluence (Fig. 2.4). Naturally, there will be some differences between the optimal and actual fluence after enforcing the mechanical constraints of the linear accelerator (e.g., maximum leaf speed, limitations in opposing leaf positions). Another strategy is the so-called Direct Aperture Optimization, where fluence optimization and leaf positions are optimized directly,12 therefore what the dosimetrist sees at the optimization stage is what is delivered to the patient. Figure 2.4: Ten reconstructed beamlets (out of total 200) in a VMAT plan. The blue shaded area is the MLCs aperture. The more irregularly shaped the beamlets, the more complex the plan. Source: Radiation Oncology Department of Papageorgiou General Hospital. 2.4 Plan complexity VMAT radiation plans are inherently complex to deliver by the linear accelerator. Plan complexity has many reverberations on radiation safety, treatment deliverability, beam-on time, mechanical strain on the linear accelerator, and necessitates laborious quality assurance (QA) and verification procedures.13 Concretely, some significant issues arising from excessive complexity include the following: As the number of monitor units (MUs) increases, so does irradiation time and leakage radiation. This low-dose bath of normal tissues may double the risk of radiation-induced second cancers.14 Increased overall treatment time might have unfavorable radiobiologic implications in cell-sterilizing ability due to repair of sublethal DNA damage.1519 The interaction between the delivery of an intricate plan and moving target (such as prostate) is, to a great degree, unknown. Complexity metrics (CMs) have been suggested as a complementary analytical tool to existing quality controls methods.20 The extra information provided by CMs could facilitate the selection of a robust plan, deliverability-wise, among plans with similar dosimetric characteristics (Fig. 2.5). A complexity metric could also be used as an optimization objective in the treatment planning system to reduce the plan complexity at an early stage.21 Integrating complexity minimization at the objective function, rather than at later stages of treatment planning, is advantageous in the following sense. It increases delivery efficiency while maintaining dosimetric quality. On the contrary, other methods, such as beamlet intensity restrictions, smoothing procedures, and direct aperture optimization, may compromise target coverage and OAR sparing in exchange for increased beam smoothness and delivery efficiency.21 Figure 2.5: Qualitative representation of complexity vs. plan quality and plan deliverability. However, a high degree of complexity is not a priori a negative feature of a treatment plan. It may be the case that due to the particular target and anatomy of organs at risk, a rather intricate plan needs to be delivered. This may occur around complex targets involving convex and non-convex parts, and a tradeoff between treatment plan quality and required monitor units has been shown in the context of IMRT.22 Therefore, a balance between excessive complexity and dose distribution compromise is the most prudent way forward. "],["literature.html", "3 Literature 3.1 Background 3.2 Circumference/Area (CoA) 3.3 Leaf Travel (LT) 3.4 Edge Metric (EM) 3.5 Equivalent Square Field (ESF) and Mean Field Area (MFA) 3.6 Modulation Complexity Score (MCS) and Leaf Travel MCS for VMAT (LTMCSV) 3.7 Modulation Index total (\\(\\text{MI}_\\text{total}\\)) 3.8 Plan Irregularity (PI) 3.9 Small Aperture Score (SAS)", " 3 Literature 3.1 Background Complexity metrics can be divided into two groups, fluence map-based and aperture-based.23 The former measure the variations of fluences intensity and have originally been described for IMRT plans with multiple static gantry angles. The latter consider MLC openings size and shape and can be applied to both IMRT and VMAT plans. For a thorough review, the reader may refer to Hernandez et al24 and Chiavassa et al.25 In this thesis, we study aperture-based metrics. Various TPSs assign different priorities when modulating the plan parameters. For example, Varian Eclipse plans are demanding for the MLC, rendering them more vulnerable to uncertainties stemming from the use of small apertures and potential errors in the MLC calibration.24 Other TPSs, such as Philips Pinnacle and Elekta Monaco, modulate the gantry speed and the dose rate on top of MLC positions. Therefore, a departments adoption of a complexity metric should be individualized, considering the differences between treatment planning systems and linear accelerator technical specifications. 3.2 Circumference/Area (CoA) The ratio of circumference over area may be calculated for each MLC aperture, via the following formula: \\[ \\text{CoA}_\\text{beam} = \\sum_{i=1}^{S} \\frac{\\text{Perimeter}_i}{\\text{Area}_i} \\] where \\(S\\) is the total number of segments. However, it would be more prudent to take into account the monitor units of the \\(i_\\text{th}\\) segment \\((\\text{MU}_i)\\). Then, the plans complexity (\\(\\text{CoA}_\\text{plan}\\)) would be written as the sum of CoA over all beams \\(B\\), weighted by the proportion of MUs delivered through the segments over the total MUs of the plan \\((\\text{MU}_\\text{plan})\\): \\[ \\text{CoA}_\\text{plan} = \\frac{1}{\\text{MU}_\\text{plan}}\\sum_{j=1}^B \\underbrace{\\sum_{i=1}^{S} \\frac{\\text{Perimeter}_i}{\\text{Area}_i} \\times \\text{MU}_i}_{\\text{CoA of }j\\text{-th beam}} \\] The advantage of this index is that it conceptually is a straight-forward and tangible metric.20 In Fig. 3.1, the CoA for a patient with locally advanced prostate cancer has been calculated. The plot uses polar coordinates to express complexity as a function of gantrys rotation. The surge in complexity between \\(19\\pi/12\\) and \\(11\\pi/6\\) exposes the plan to delivery uncertainties, such as intrafraction prostate motion. Ideally, complexity should be spread out over the entire arc. However, this is not always feasible due to the geometric relation of organs at risk and planning treatment volumes. Figure 3.1: Polar plot of complexity (circumference/area) as a function of gantry angle in a patient with locally advanced prostate cancer. The shaded part of the arcs is associated with a high degree of complexity, rendering the plan vulnerable to spatio-temporal uncertainties (e.g., intrafraction prostate motion). Source: Radiation Oncology Department of Papageorgiou General Hospital. 3.3 Leaf Travel (LT) This index measures the average distance traveled by the moving leaves of the collimator. LT was conceived for VMAT treatments consisting of a single full arc. To allow for straightforward comparisons between plans with different arcs or partial arcs, LT is sometimes divided by the corresponding arc length (typically about 360 degrees for single arcs and about 720 degrees for double arcs). In this thesis, we calculated the plans total LT by weighting each arcs LT based on its monitor units.1 \\[ \\text{LT}_{\\text{beam}} = \\frac{1000(mm) - \\text{LT Mean}(mm)}{1000(mm)} \\] The plans total LT is the weighted sum of beams LT over all \\(B\\) beams: \\[ \\text{LT}_{\\text{plan}} = \\frac{1}{\\text{MU}_\\text{plan}}\\sum_{i=1}^B \\text{LT}_\\text{beam} \\times \\text{MU}_i \\] 3.4 Edge Metric (EM) This metric describes the complexity of multileaf collimator apertures based on the ratio of leaf side edge length and the distances between leaves. The constants \\(C_1\\) and \\(C_2\\) in the formula below constitute scaling factors that define the relative importance of the \\(x\\) and \\(y\\) edges.26 \\(A\\) is the segments area. As the differences between adjacent leaves positions accumulate, the higher the values of EM index are assumed. EM is inherently linked to the tongue-and-groove effect.26 In the original study by Younge et al., the recommended parameters were \\(C_1 = 0\\) and \\(C_2 = 1\\) because this combination produced the strongest correlation between EM and the measured dosimetric error. \\(W_i\\) are the aperture weights that, in their absence, the optimization will be biased towards the regularization of apertures with the lowest number of MUs because any change in these apertures will have the least dosimetric impact. \\[ \\text{EM}_\\text{beam} = \\sum_{i=1}^S W_i \\frac{C_1 x_i + C_2 y_i}{A_i} \\] In Fig. 3.2, various segments apertures are shown when the edge metric penalty is enabled or disabled in the cost function \\(J\\). The penalty is recalculated after each iteration during the optimization process and added to the dose-related component of the cost function to yield the total cost. A scaling factor, expressed as the relative importance of EM penalty with respect to the dose-related cost function, is used, i.e. \\(J_\\text{EM} = \\lambda \\text{EM}_\\text{plan} = \\lambda \\sum_{i=1}^S W_i y_i/A_i\\). In Fig. 3.3, the dose difference between the theoretically calculated and measured doses is shown (\\(\\lambda = 0.031\\)). It is evident how the activation of edge penalty regularizes apertures shapes and reduces the dosimetric error. Figure 3.2: Aperture-regularization integration into the objective function of VMAT optimization process. Top: Apertures generated without an edge penalty. Bottom: Apertures generated with an edge penalty. Source: Younge et al.[26}. Figure 3.3: (a) Dose difference (calculated  measured) in a VMAT plan without the edge penalty. (b) With the edge penalty turned on. (c) Distribution of measured dose deviations. Source: Younge et al.26. 3.5 Equivalent Square Field (ESF) and Mean Field Area (MFA) ESF is defined as the square root of the total area of the beamss apperture. In literature, a conversion function, \\(f(x) = 1-e^{-x}\\), has been used to endow distances with a non-linear contribution to the fields complexity.20 In this thesis, distances contributed equally to the ESF score. A similar complexity metric is the mean field area, that was found to correlate with gamma passing rates in prostate IMRT plans.27,28 3.6 Modulation Complexity Score (MCS) and Leaf Travel MCS for VMAT (LTMCSV) This score considers two contributing factors to a plans complexity. Namely, the variability in the shape of beamlets (Leaf Sequence Variability, LSV) and the geometric variation of their aperture area (Aperture Area Variability, AAV).29 The formula for calculating the MCS score of a plan is as following: \\[ \\text{MCS}_\\text{plan} = \\sum_{j=1}^{B} \\left[\\underbrace{\\left(\\sum_{i=1}^{S} \\text{AAV}_i \\times \\text{LSV}_i \\times \\frac{\\text{MU}_i}{\\text{MU}_{\\text{beam,}j}}\\right)}_{\\text{MCS of } j \\text{-th beam}} \\times \\frac{\\text{MU}_{\\text{beam,}j}}{\\text{MU}_\\text{plan}} \\right] \\] \\[\\begin{align*} LSV_j &amp;= \\left[\\frac{\\sum_{i=1}^{N-1} p_\\text{max}-(p_i-p_{i+1})}{(N-1) \\cdot p_\\text{max}}\\right]_{\\text{left bank},j} \\times \\left[\\frac{\\sum_{i=1}^{N-1} p_\\text{max}-(p_i-p_{i+1})}{(N-1) \\cdot p_\\text{max}}\\right]_{\\text{right bank},j}\\\\ AAV_j &amp;= \\frac{ \\sum_{i=1}^N p_{i,_{\\text{left bank}}} - p_{i,_{\\text{right bank}}}} {N\\cdot\\left(\\text{max}(p_{i,\\text{left bank}})-\\text{max}(p_{i,\\text{right bank}})\\right)} \\end{align*}\\] where \\(p_i\\) is the coordinate of the \\(i_\\text{th}\\) leaf position, and \\(p_\\text{max}\\) is the maximum distance between positions for a given leaf bank, summed over all \\(B\\) beams (arcs) and all segments \\(S\\) (Fig. 3.4). MCS values range in \\([0,1]\\), and by the definition above, it follows that the higher the MCS value, the lower the complexity. MCS enables the comparison of complexity between different treatment sites and the comparison of single vs. multiple arcs plans. The largest value of MCS, 1.0, is realized by an open field, since \\(p_i = p_{i+1}\\): Figure 3.4: Schematic representation of an apertures shape. In both the left and the right banks the extreme right and extreme left leaves are in yellow and red, respectively. The maximum leaf opening, \\(p_\\text{max}\\), corresponds to the the red leaf in the left bank and in the yellow leaf in the right bank. Modified from: Sumida et al.30 \\[\\begin{align*} LSV_j &amp;= \\left[\\frac{\\sum_{i=1}^{N-1} p_\\text{max}}{(N-1) p_\\text{max}}\\right]_{\\text{left bank},j} \\left[\\frac{\\sum_{i=1}^{N-1} p_\\text{max}}{(N-1) p_\\text{max}}\\right]_{\\text{right bank},j} = \\frac{(N-1) p_\\text{max}}{(N-1)p_\\text{max}} \\frac{(N-1) p_\\text{max}}{(N-1)p_\\text{max}} = 1\\\\ AAV_j &amp;= \\frac{ \\sum_{i=1}^N p_\\text{max}} {N p_\\text{max}} = 1 \\end{align*}\\] Therefore: \\[\\begin{align*} \\text{MCS}_\\text{plan} &amp;=\\sum_{j=1}^B \\left[\\left(\\sum_{i=1}^S \\frac{\\text{MU}_i}{\\text{MU}_{\\text{beam},_j}}\\right) \\times \\frac{\\text{MU}_{\\text{beam},_j}}{\\text{MU}_\\text{plan}} \\right] = \\sum_{j=1}^B \\left[\\left(\\frac{1}{\\text{MU}_{\\text{beam},_j}} \\sum_{i=1}^S \\text{MU}_{i}\\right) \\times \\frac{\\text{MU}_{\\text{beam},_j}}{\\text{MU}_\\text{plan}} \\right]\\\\ &amp;=\\sum_{j=1}^B \\left(\\frac{\\text{MU}_{\\text{beam},_j}}{\\text{MU}_{\\text{beam},_j}} \\times \\frac{\\text{MU}_{\\text{beam},_j}}{\\text{MU}_\\text{plan}}\\right) =\\frac{1}{\\text{MU}_\\text{plan}} \\sum_{j=1}^B \\text{MU}_{\\text{beam},j} = \\frac{\\text{MU}_\\text{plan}}{\\text{MU}_\\text{plan}} = 1 \\end{align*}\\] The original MCS formulation was referring to step and shoot IMRT. However, during a VMAT arc, MUs are delivered in a continuous manner between adjacent control points (CPs). Thus, the calculation of MCS has been slightly modified to adapt to the VMAT case.31 Concretely, we must consider the product of the mean values of \\(\\text{LSV}_i\\) and \\(\\text{AAV}_i\\) between adjacent control point: \\[ \\text{MCS-VMAT}_\\text{beam} = \\sum_{i=1}^{S-1}\\left( \\frac{\\text{AAV}_i + \\text{AAV}_{i+1}}{2} \\times \\frac{\\text{LSV}_i + \\text{LSV}_{i+1}}{2} \\times \\frac{\\text{MU}_{i,i+1}}{\\text{MU}_\\text{beam}}\\right) \\] Although MCS is capable of distinguishing between the levels of complexity of different treatment sites, it has not yet been validated as a surrogate marker of dosimetric deviations for specific treatment sites. A multiplicative combination of mean leaf travel and modulation complexity score has also been proposed, i.e. \\(\\text{LTMCS} = \\text{LT} \\times \\text{MCS}\\),31 or \\(\\text{LTMCSV} = \\text{LT} \\times \\text{MCSV}\\) for the case of a VMAT plan. 3.7 Modulation Index total (\\(\\text{MI}_\\text{total}\\)) This index involves the variations in speed and acceleration of the MLC, as well as variations of the gantry speed (GA) and the dose rate (DRV).32 Its calculation is based on the concept of Modulation Index introduced by Webb.33 First, we define a counting function \\(z_\\text{total}(f)\\): \\[\\begin{align*} z_\\text{total}(f) &amp;=\\frac{1}{S-2} \\sum_{i=1}^{S} \\bigg[N_i\\Big( \\text{MLC speed}_i&gt;f \\sigma_{\\text{MLC speed}} \\text{ or } \\text{MLC accel}_i &gt; \\alpha f \\sigma_{\\text{MLC}_\\text{accel}} \\Big) \\\\ &amp;\\cdot W_{\\text{GA},i+1} \\cdot W_{\\text{MU},i+1}\\bigg] \\end{align*}\\] where \\(S\\) is the total number of control points and: \\[N_i\\Big( \\text{MLC speed}_i&gt;f \\sigma_{\\text{MLC speed}} \\text{ or } \\text{MLC accel}_i &gt; \\alpha f \\sigma_{\\text{MLC}_\\text{accel}}\\Big)\\] means that if the MLC speed or acceleration at the \\(i_\\text{th}\\) control point is larger than \\(f\\sigma_{\\text{MLC speed}}\\) or \\(\\alpha f \\sigma_{\\text{MLC}_\\text{accel}}\\), respectively, then the value of that CP becomes \\(1\\) multiplied by the weight factors \\(W_{\\text{GA},i+1}\\) and \\(W_{\\text{MU},i+1}\\), \\(\\sigma_{\\text{MLC speed}_i}, \\sigma_{\\text{MLC accel}_i}\\) being the standard deviations of MLC \\(\\text{speed}_i\\) and MLC \\(\\text{accel}_i\\), respectively. The \\(f\\) argument controls therefore the threshold beyond which a leaf pair contributes to complexity, and, similarly, \\(\\alpha\\) is a scaling factor that empirically is set to \\(\\alpha = 1/\\text{Time}_i\\). The weight factors are defined by the following formulas: \\[\\begin{align*} {}&amp;W_{\\text{GA,}_{i+1}} = \\frac{\\beta}{1+(\\beta-1)\\exp\\left(-\\frac{\\text{GA}_i}{\\gamma}\\right)}\\\\ {}&amp;W_{\\text{MU,}_{i+1}} = \\frac{\\beta}{1+(\\beta-1)\\exp\\left(-\\frac{\\text{DRV}_i}{\\gamma}\\right)} \\end{align*}\\] In the paper of Park et al.32 it is \\(\\beta = \\gamma = 2\\), and the gantry acceleration \\(\\text{GA}_i\\) and dose rate variation \\((\\text{DRV}_i)\\) are calculated as: \\[\\begin{align*} \\text{GA}_i &amp;= \\left| \\frac{\\Delta\\text{Gantry angle}_i}{\\text{Time}_i} - \\frac{\\Delta\\text{Gantry angle}_{i+1}}{\\text{Time}_{i+1}} \\right|\\\\ \\text{DRV}_i &amp;= \\left|\\text{DR}_i - \\text{DR}_{i+1}\\right| \\end{align*}\\] Finally, \\(\\text{MI}_\\text{total}\\) is calculated as the sum of the contributions of all the individual leaves: \\[ \\text{MI}_t = \\sum_{i=1}^{N_\\text{leaves}} \\int_0^k z_\\text{total}(f)\\mathrm{d}f, \\hspace{1cm} k = 0.2, 0.5, 1 \\text{ and } 2 \\] 3.8 Plan Irregularity (PI) PI quantifies the deviations of aperture shapes from a circle,23 with \\(\\text{PI}=1\\) corresponding to a perfect circle. In Fig. 3.5 aperture area (AA) of beam \\(b\\) and segment \\(s\\) is given by: \\[ \\text{AA}_{bs} = \\sum_{k=1}^{N_{\\small\\text{LP}}} t_k\\left(x_{bsk}^{(2)} - x_{bsk}^{(1)}\\right) \\] Where \\(k\\) is the leaf pair index, \\(N_\\text{LP}\\) is the total number of leaf pairs, \\(t_k\\) is the width of the \\(k_\\text{th}\\) leaf pair and \\(x^{(1)}, x^{(2)}\\) are the positions of the leaves of te pair. The aperture irregularity is computed as: \\[ \\text{AI}_{bs} = \\frac{\\text{AP}_{bs}^2}{4\\pi \\text{AA}_{bs}} \\] Figure 3.5: Schematic representation of beam aperture analysis for the plan irregularity index. For instance, for a square segment \\(\\text{AP} = 4a, \\text{AA} = a^2\\). Then: \\[ \\text{AI}_{bs} = \\frac{16a^2}{4\\pi a^2} = \\frac{4}{\\pi} \\simeq 1.273 \\] On the contrary, for a perfect circular segment it holds that \\(\\text{AP} = 2\\pi R, \\text{AA} = \\pi R^2\\). Then: \\[ \\text{AI}_{bs} = \\frac{4\\pi^2 R^2}{4\\pi \\cdot\\pi R^2} = 1 \\] Given the quantities above one can define beam irregularity (BI) of beam \\(b\\) averaged over all \\(S\\) segments: \\[ \\text{BI}_{b} = \\frac{\\sum_{s=1}^S\\text{MU}_{bs} \\text{AI}_{bs}}{\\text{MU}_{b}} \\] And the plan irregularity (PI) averaged over all \\(B\\) beams: \\[ \\text{PI} = \\frac{\\sum_{b=1}^B\\text{MU}_{b} \\text{BI}_{b}}{\\text{MU}_{\\text{plan}}} \\] 3.9 Small Aperture Score (SAS) The small aperture score (SAS) is defined as the ratio of open leaves for which the aperture is less than a predefined threshold (e.g., 2, 5, 10mm) to all open leaf pairs: \\[ \\text{SAS}(x)_{\\text{beam}} =\\sum_{i=1}^S \\left[\\frac{N(x&gt;a&gt;0)_i}{N(a&gt;0)_i} \\times \\frac{\\text{MU}_i}{\\text{MU}_\\text{beam}}\\right] \\] Where \\(x\\) is the aperture criteria, e.g., \\(x = \\{2, 5, 10\\}\\), \\(S\\) is the number of segments in the beam, \\(N\\) is the number of leaf pairs not blocked by the jaws, and \\(a\\) is the aperture distance between opposing leaves. The \\(\\text{SAS}(5\\text{mm})\\) metric has been found to correlate well with \\(\\Gamma(2\\%/2\\text{mm})\\) pass rates.27 Also, \\(\\text{SAS}(5\\text{mm})\\) was able to provide a threshold below which all plans passed their MapCHECK diode array QA tests, and above which only a small percentage of verified plans were identified, incorrectly, as likely to fail the QA process. There were no plans with partial arcs in our dataset. "],["methods.html", "4 Methods 4.1 Dataset curation 4.2 Dataset description 4.3 Data analysis", " 4 Methods 4.1 Dataset curation Two-hundred and seventeen RT-DICOM2 files of VMAT prostate plans were exported from the Varian ARIA (Varian Medical Systems, Palo Alto, CA) oncology information system of the Radiation Oncology Department at Papageorgiou General Hospital. All patients were treated radically during 2017-2020, with a moderately hypofractionated radiotherapy scheme.34,35 Depending on a patients risk, the irradiation field encompassed prostate only (PO), prostate plus seminal vesicles (PSV), or the whole-pelvis (WPRT) (Fig. 4.1). Target volumes and organs at risk were delineated on the reference CT image according to the International Commission of Radiation Unit and Measurement 50 and 62 recommendations.36,37 PTVs were derived from clinical target volumes with an anisotropic expansion (7mm towards all directions except 5mm towards the rectum). Figure 4.1: Images of 2D dose distributions for prostate plans of various field sizes. From left to right: prostate plus seminal vesicles (PSV), prostate only (PO), and whole pelvis (WPRT). Source: Radiation Oncology Department of Papageorgiou General Hospital. The plans were created with the Varian Eclipse Treatment Planning System. During the study period, the TPS has undergone a major upgrade from version 11.0.47 to version 15.1.52, which affected the fluence optimization and leaf sequencing algorithms. All VMAT plans were delivered by a 6MV energy linear accelerator (DHX, Varian Medical Systems, Palo Alto, CA) equipped with a 120-leaf MLC (field size 40 \\(\\times\\) 40cm, central 20cm of field - 5mm leaf width, outer 20cm of field - 10mm leaf width), and an on-board imaging kV cone-beam CT system. The variables of interest were extracted with in-house software under the name rteval (http://test.ogkologia.gr/rteval). The latter can extract a diverse set of radiotherapy parameters from RTDOSE, RTPLAN, and RTSTRUCT DICOM files, such as point dose statistics, clinical dose constraints, arbitrary dose-volume data, contour coordinates, various complexity metrics, and others. The software is written in Python 3 and uses the pydicom library (version 1.3.0). All data were fully anonymized to protect patient anonymity. The research protocol was submitted to the Institutional Review Board of Papageorgiou General Hospital. 4.2 Dataset description The following data were extracted: Variables included in the dataset, their type, and a short description of their meaning. Variable Name Type Description coa Numerical Circumference over Area (1/mm) em Numerical Edge Metric (1/mm) esf Numerical Equivalent Square Field (mm) lt Numerical Leaf Travel ltmcsv Numerical Leaf Travel multiplied by MCS-VMAT mcs Numerical Modulation Complexity Score mcsv Numerical Modulation Complexity Score for VMAT plans mfa Numerical Mean Field Area (\\(\\text{mm}^2\\)) pi Numerical Plan Irregularity sas Numerical Small Aperture Score at 5 mm total_mus Numerical Total number of monitor units of the plan arcs Categorical The number of beams (1 or 2) field_size Categorical Prostate Only, Prostate plus Seminal Vesicles and Whole Pelvis RT physician Categorical The physician that delineated the tumor volume and organs at risk dosimetrist Categorical The dosimetrist that performed the treatment planning tps_version Categorical The version of Treatment Planning Software cn Numerical Conformation number indicating how tightly dose conforms to target ptv_72 Numerical The high-dose (72 Gy) planning treatment volume bladder_v65 Numerical The volume of bladder that receives at least 65 Gy bladder_v70 Numerical The volume of bladder that receives at least 70 Gy rectum_v60 Numerical The volume of rectum that receives at least 60 Gy rectum_v65 Numerical The volume of rectum that receives at least 65 Gy rectum_v70 Numerical The volume of rectum that receives at least 70 Gy Complexity metrics. The complexity metrics have been described in the literature section of the thesis. Total monitor units. This is the sum of the monitor units of all plans beams. Physician. In total, seven physicians delineated tumor volumes and organs at risk. Their usernames haven been replaced by the string \\(\\text{Phys}_i, i = 1,\\ldots,7\\) to protect anonymity. Dosimetrist. In total, two dosimetrists optimized the plans. Their usernames have been replaced by the string \\(\\text{Dos}_i, i = \\{1,2\\}\\) to protect anonymity. TPS version. Plans from two different versions of the Varian Eclipse TPS entered the study, version 11.0.47 and version 15.1.52. Conformation Number (CN). This index measures how tightly the dose distribution conforms around the target.38 It is given by the following equation: \\[ \\text{CN} = \\frac{\\text{TV}_{RI}}{\\text{TV}} \\times \\frac{\\text{TV}_{RI}}{\\text{V}_{RI}} \\] where \\(\\text{TV}\\) corresponds to the target volume, \\(\\text{TV}_{RI}\\) to the target Volume covered by some reference isodose (RI), and \\(\\text{V}_{RI}\\) to the volume of the reference isodose. CN consists of two factors, with the first fraction describing the quality of target coverage, whereas the second one the volume of normal tissue receiving a dose equal or greater to a reference dose. A value of 1 represents a reference isodose covering the target volume without irradiation of normal tissues and corresponds to optimal conformation. A value of zero implies a total absence of conformation. We used 95% of the prescribed dose level for the high-dose (72 Gy) PTV as the reference dose to apply the vant Reits formula.38 High-dose (72 Gy) PTV. In our department, a moderately hypofractionated scheme is used, where the prostate is receiving 72 Gy in 32 fractions with 2.25 Gy/fraction, seminal vesicles 57.6 Gy with 1.8 Gy/fraction, and pelvic lymph nodes 51.2 Gy with 1.6 Gy/fraction. Dose-volume constraints. These constraints allow only a certain percentage of an organs volume (e.g., bladder or rectum) to receive doses above some prespecified threshold level. They have for some time been considered as a de facto standard clinical tool for plan evaluation.39 4.3 Data analysis After their extraction, the data were tabulated in a Microsoft Excel spreadsheet. The statistical analysis was done in R language version 3.6.3.40 Some prototyping and exploratory analyses were performed with Wolfram Mathematica version 12.1, Python version 3.6.9, and scikit-learn version 0.23. The results of the principal component analysis were independently verified with IBM SPSS version 25. This thesis was written in Bookdown,41,42 an R package built on top of R Markdown, that allows reproducible research. Unless otherwise specified in the text, the level of statistical significance was set at a=0.05. 4.3.1 Summary statistics We analyzed each variable via the Shapiro-Wilk normality test, descriptive statistics (mean/median, skewness, kurtosis), histograms, and Q-Q plots to generate a summary of the data. All variables deviated from normality; therefore, their values are reported in median and interquartile ranges (IQR). 4.3.2 Principal Component Analysis (PCA) The set of all complexity metrics was analyzed with principal component analysis. The assumptions of PCA were checked with the correlation matrix, and the level of correlation that was considered worthy of a variables inclusion in the PCA was \\(r \\ge 0.3\\). I.e., for a variable to be included in further analysis, it needed to have a correlation coefficient \\(r \\ge 0.3\\) with at least one of the rest variables. Sampling adequacy was checked with the Kaiser-Meyer-Olkin (KMO) measure for the overall dataset, and with the Measure of Sampling Adequacy (MSA) for each individual metric,43 along with Bartletts test of sphericity.3 For KMO and MSA calculations we used the R package REdaS version 0.9.3. All data were centered and scaled before applying PCA. The PCA results include the report of communalities4 of the metrics. The principal components extraction was based on the eigenvalue criterion and the scree plots. 4.3.3 Mutual information analysis (MIA) For every pair of complexity metrics the mutual information (MI), \\(I(X;Y)\\), was calculated via the following formula: \\[ \\operatorname{I}(X;Y) = \\int_{S_{\\mathcal Y}} \\int_{S_{\\mathcal }} {p_{\\scriptsize{XY}}(x,y) \\log{ \\left(\\frac{p_{\\scriptsize{XY}}(x,y)}{p_{\\scriptsize X}(x)\\,p_{\\scriptsize Y}(y)} \\right) } } \\; \\mathrm{d}x \\, \\mathrm{d}y \\] The probability density functions \\(p_{\\scriptsize{X}}(x),p_{\\scriptsize{Y}}(y),p_{\\scriptsize{XY}}(x,y)\\) were approximated with a smooth kernel distribution, given by a linearly interpolated version of a Gaussian kernel: \\[ \\hat{f}(x;h) =\\frac{1}{n}\\sum_{i=1}^n \\underbrace{\\frac{1}{h}\\frac{1}{\\sqrt{2\\pi}}\\text{exp}\\left[-\\frac{1}{2}\\left(\\frac{x-x_i}{h}\\right)^2\\right]}_{K_h(x-x_i)} \\] Where \\(K_h\\) is the scaled kernel \\(K_h(u) = \\frac{1}{h}K\\left(\\frac{u}{h}\\right)\\) and \\(K(u) = \\frac{1}{\\sqrt{2\\pi}}\\text{exp}(-u^2/2)\\). The bandwidth \\(h\\) was estimated with the Silvermans rule. In analogy with the univariate case, the bivariate kernel density estimate was calculated via: \\[ \\hat{f}(\\mathbf{x}; \\mathbf{H}) = \\frac{1}{n}\\sum_{i=1}^n K_{\\mathbf{H}} \\left(\\mathbf{x} - \\mathbf{x}_i\\right) \\] Where \\(\\mathbf{H}\\) is a symmetric and positive definite bandwidth matrix and \\(K_{\\mathbf{H}}(\\mathbf{u}) = |\\mathbf{H}|^{-1/2}K(\\mathbf{H}^{-1/2}\\mathbf{u})\\) and \\(K(\\mathbf{u}) = \\frac{1}{2\\pi} \\text{exp}\\left(-\\frac{1}{2}\\mathbf{u}^ \\mathbf{u}\\right)\\). Unlike the correlation coefficient, mutual information is more general and captures how much the joint distribution of the pair \\(p_{\\scriptsize{XY}}(x,y)\\) differs to the product of the marginal distributions of \\(p_{\\scriptsize{X}}(x)\\) and \\(p_{\\scriptsize{Y}}(y)\\). In the limit where \\(X\\) and \\(Y\\) are completely independent, it holds that \\(p_{\\scriptsize{XY}}(x,y)=p_{\\scriptsize{X}}(x)\\cdot p_{\\scriptsize{Y}}(y)\\). Therefore \\(\\text{log}\\left(\\frac{p_{\\scriptsize{XY}}(x,y)}{p_{\\scriptsize{X}}(x)\\cdot p_{\\scriptsize{Y}}(y)}\\right) = \\text{log}\\left(\\frac{p_{\\scriptsize{X}}(x)\\cdot p_{\\scriptsize{Y}}(y)}{p_{\\scriptsize{X}}(x)\\cdot p_{\\scriptsize{Y}}(y)}\\right)=\\text{log}(1)=0\\). 4.3.4 Linear regression analysis Traditionally, the minimum required sample size for linear regression models was ten subjects per variable or 200 subjects for any regression analysis. In a recent paper by Riley et al.[44}, a methodology was proposed to calculate the minimum sample size needed to develop a prediction model using linear regression. The authors describe four criteria that need to be fulfilled: small overfitting, small absolute difference in the models apparent and adjusted R-squared value, precise estimation of the residual standard deviation, and precise estimation of the average outcome value. They also developed the pmsampsize package in R that we used. In our dataset the average value of complexity was \\(\\overline{c} = 0.150\\) and the standard deviation \\(s_c = 0.037\\). Assuming an \\(R^2\\) of \\(0.3\\), and by including seven predictor parameters in the model, the necessary sample size is calculated as follows: library(pmsampsize) pmsampsize(type = &quot;c&quot;, rsquared = 0.3, parameters = 7, shrinkage = 0.9, intercept = 0.150, sd = 0.037, mmoe = 1.1) ## NB: Assuming 0.05 acceptable difference in apparent &amp; adjusted R-squared ## NB: Assuming MMOE &lt;= 1.1 in estimation of intercept &amp; residual standard deviation ## SPP - Subjects per Predictor Parameter ## ## Samp_size Shrinkage Parameter Rsq SPP ## Criteria 1 120 0.900 7 0.3 17.14 ## Criteria 2 99 0.883 7 0.3 14.14 ## Criteria 3 241 0.946 7 0.3 34.43 ## Criteria 4* 242 0.946 7 0.3 34.57 ## Final 242 0.946 7 0.3 34.57 ## ## Minimum sample size required for new model development based on user inputs = 242 ## ## * 95% CI for intercept = (0.17, 0.13), for sample size n = 242 Based on this calculation, at least \\(N=242\\) VMAT plans are needed for conducting regression analysis. The datasets size was \\(N=217\\); therefore, we recognize that our analysis might be marginally underpowered. We assessed multicollinearity, both visually via a matrix scatter plot and numerically, by calculating the variance inflation factors (VIFs).45 The cutoff VIF value for omitting a variable was 4. After excluding collinear variables, we performed a univariate analysis on the rest of the covariates to decide which one would be included in the multivariate regression analysis. Any variable with \\(p &lt; 0.2\\) in the univariate analysis was included in multivariate analysis. We also developed two models via backward and forward selection methods. Backward stepwise selection starts with the full model containing all predictor variables. Then, the least useful predictor is iteratively removed, one at a time. The single best model at each time is selected via the Akaike information criterion (AIC). The latter is an estimator of out-of-sample prediction error and evaluates each models quality, relative to each of the other models. It is given by the formula \\(\\mathrm{AIC} \\, = \\, 2k - 2\\ln(\\hat L)\\), where \\(k\\) is the number of estimated parameters in the model and \\(\\hat{L}\\) the maximum value of the models likelihood function. On the contrary, forward stepwise selection starts with a model that contains no predictors at all. Subsequently, it adds predictors one at a time until all of the predictors have been considered. At each step, the predictor that gives the most considerable additional improvement to the fit is added. The final models validity was verified with the following diagnostic plots: residuals vs. fitted plot (linearity of relationship), normal Q-Q plot (normality in the residuals), scale-location plot (equality of variances), and the residuals vs. leverage plot. 4.3.5 Logistic regression analysis Instead of predicting the exact value of complexity, we dichotomized VMAT plans into high complexity and low complexity ones. Normally, the threshold choice should be based on an analysis of complexity vs. deliverability. Concretely, plans designated as low complexity should correlate with higher scores in quality assurance and verification procedures (e.g., gamma index analysis). However, the determination of the optimal cutoff value is beyond the thesiss scope. Instead, we arbitrarily defined as high complexity plans, those whose complexity exceeded the datasets median value. After this division, the problem converted from prediction to classification and became amenable to logistic regression analysis. We modeled the probability of predicting low complexity given some values of \\(X\\), \\(p(X)=\\text{Pr}(Y=1|X)\\), using the logistic function that gives outputs 0 and 1 for all \\(X\\): \\[ p(X) = \\frac{e^{\\beta_0 + \\beta^ X}}{1 + e^{\\beta_0 + \\beta^ X}} \\] The model included all candidate variables, and to fit it, we used LASSO5 regularization that is known to invoke sparsity-based feature selection. The R package we used was glmnet46 version 4.0.2, which solves the following optimization problem: \\[ \\min_{(\\beta_0, \\beta) \\in \\mathbb{R}^{p+1}} \\overbrace{-\\left[\\frac{1}{N} \\sum_{i=1}^N y_i (\\beta_0 + x_i^ \\beta) - \\log (1+e^{\\beta_0+x_i^ \\beta})\\right]}^{\\text{negative binomial log-likehood}} + \\underbrace{\\lambda \\big[ (1-\\alpha)\\|\\beta\\|_2^2/2 + \\alpha\\|\\beta\\|_1\\big]}_{\\text{elastic-net penalty}} \\] over some range of \\(\\lambda\\) values. The hyperparameter \\(\\alpha\\) controls the elastic-net penalty, ranging from \\(\\alpha=1\\) for LASSO and \\(\\alpha = 0\\) for the ridge. In our case, we used \\(\\alpha = 1\\). We split the dataset into a training set (75%) and a test set (25%). The optimal value of the regularization parameter \\(\\lambda\\) was determined through 5-fold cross-validation in the training set (Fig. 4.2). Subsequently, the best LASSO model, corresponding to the optimal value of \\(\\lambda\\), was evaluated to the test set through ROC and confusion matrix analysis. Figure 4.2: Schematic representation of k-fold cross-validation (\\(k=5\\)). The training set is split into \\(k\\) smaller sets. The model is trained using \\(k-1\\) folds and validated on the remaining data. The best model is evaluated in the holdout test set. Modified from scikit documentation. RT-DICOM is an extension of the DICOM standard applied to radiation therapy. Bartletts test checks whether a matrix is significantly different from the identity matrix. When applied to a correlation matrix, it provides a statistical probability that there exist significant correlations among at least some of the variables. For factor analysis, some relationships are necessary between variables; therefore, a significant Barletts test of sphericity is required, e.g., \\(p &lt; 0.001\\). Communality is the proportion of each variables variance accounted for by the principal components. Communalities for the \\(i_\\text{th}\\) variable are computed by taking the sum of the squared loadings for that variable, i.e. \\(\\hat{h}_i^2=\\sum_{j=1}^m \\hat{\\varphi}_{ij}^2\\). LASSO stands for least absolute shrinkage and selection operator. "],["results.html", "5 Results 5.1 Summary of the data 5.2 Principal Component Analysis 5.3 Mutual Information Analysis 5.4 Linear Regression Analysis 5.5 Logistic Regression analysis", " 5 Results 5.1 Summary of the data html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #svqzaebjer .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #svqzaebjer .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #svqzaebjer .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #svqzaebjer .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #svqzaebjer .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #svqzaebjer .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #svqzaebjer .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #svqzaebjer .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #svqzaebjer .gt_column_spanner_outer:first-child { padding-left: 0; } #svqzaebjer .gt_column_spanner_outer:last-child { padding-right: 0; } #svqzaebjer .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #svqzaebjer .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #svqzaebjer .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #svqzaebjer .gt_from_md > :first-child { margin-top: 0; } #svqzaebjer .gt_from_md > :last-child { margin-bottom: 0; } #svqzaebjer .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #svqzaebjer .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #svqzaebjer .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #svqzaebjer .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #svqzaebjer .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #svqzaebjer .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #svqzaebjer .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #svqzaebjer .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #svqzaebjer .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #svqzaebjer .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #svqzaebjer .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #svqzaebjer .gt_sourcenote { font-size: 90%; padding: 4px; } #svqzaebjer .gt_left { text-align: left; } #svqzaebjer .gt_center { text-align: center; } #svqzaebjer .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #svqzaebjer .gt_font_normal { font-weight: normal; } #svqzaebjer .gt_font_bold { font-weight: bold; } #svqzaebjer .gt_font_italic { font-style: italic; } #svqzaebjer .gt_super { font-size: 65%; } #svqzaebjer .gt_footnote_marks { font-style: italic; font-size: 65%; } Summary table Complexity metrics of prostate VMAT plans Characteristic Field Size p-value2 PO, N = 341 PSV, N = 781 WPRT, N = 1051 CoA (1/mm) 0.38 (0.05) 0.38 (0.10) 0.46 (0.06) EM (1/mm) 0.268 (0.011) 0.267 (0.010) 0.266 (0.009) 0.038 ESF (mm) 26 (6) 32 (10) 53 (8) LT 0.82 (0.06) 0.77 (0.08) 0.61 (0.10) LTMCSV 0.20 (0.05) 0.16 (0.04) 0.12 (0.03) MCS 0.234 (0.046) 0.218 (0.029) 0.206 (0.027) MCSV 0.234 (0.046) 0.217 (0.029) 0.205 (0.026) MFA (mm²) 2,278 (830) 2,766 (1,299) 5,962 (1,100) PI 23 (4) 26 (10) 49 (8) SAS (5mm) 0.21 (0.10) 0.21 (0.08) 0.15 (0.06) PO: Prostate Only, PSV: Prostate plus Seminal Vesicles, WPRT: Whole Pelvis Radiation Therapy 1 Median (IQR) 2 Kruskal-Wallis rank sum test Fig. 5.1 shows the probability density function distributions, based on a smooth kernel density estimate, for the various complexity metrics. The data have been standardized by subtracting the mean and dividing by the standard deviation. There are some evident similarities, e.g., among the bimodal distributions of ESF, MFA and PI. Also, MCS and MCSV are practically identical. In the next section, these correlations are exploited through the principal component analysis to yield a strong dimensionality reduction. Figure 5.1: Probability density function distributions for the various complexity metrics. The data have been normalized by subtracting the mean and dividing by the standard deviation. In Table ?? the candidate predictor variables are summarized, broken down by the fields size. Whole pelvis radiation plans usually consist of two arcs, contrary to limited portals delivered with single arc plans. The dataset was unbalanced in terms of the treating physician, because PCa patients are treated mainly by the two senior physicians in our department. Rectums \\(V_{50}\\) was larger in WPRT as opposed to PSV or PO plans. By irradiating the pelvic lymph nodes, a larger part of the rectum inevitably receives at least 50 Gy.6 However, higher doses of radiation conform tightly to the prostate, which explains why rectums \\(V_{70}\\) does not differ significantly between groups. html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #wwszxbumsv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #wwszxbumsv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wwszxbumsv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #wwszxbumsv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #wwszxbumsv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wwszxbumsv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wwszxbumsv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #wwszxbumsv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #wwszxbumsv .gt_column_spanner_outer:first-child { padding-left: 0; } #wwszxbumsv .gt_column_spanner_outer:last-child { padding-right: 0; } #wwszxbumsv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #wwszxbumsv .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #wwszxbumsv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #wwszxbumsv .gt_from_md > :first-child { margin-top: 0; } #wwszxbumsv .gt_from_md > :last-child { margin-bottom: 0; } #wwszxbumsv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #wwszxbumsv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #wwszxbumsv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wwszxbumsv .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #wwszxbumsv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wwszxbumsv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #wwszxbumsv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #wwszxbumsv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wwszxbumsv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wwszxbumsv .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #wwszxbumsv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wwszxbumsv .gt_sourcenote { font-size: 90%; padding: 4px; } #wwszxbumsv .gt_left { text-align: left; } #wwszxbumsv .gt_center { text-align: center; } #wwszxbumsv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #wwszxbumsv .gt_font_normal { font-weight: normal; } #wwszxbumsv .gt_font_bold { font-weight: bold; } #wwszxbumsv .gt_font_italic { font-style: italic; } #wwszxbumsv .gt_super { font-size: 65%; } #wwszxbumsv .gt_footnote_marks { font-style: italic; font-size: 65%; } Summary table Candidate predictor variables Characteristic Field Size p-value2 PO, N = 341 PSV, N = 781 WPRT, N = 1051 Number of arcs 1 28 / 34 (82%) 47 / 78 (60%) 15 / 105 (14%) 2 6 / 34 (18%) 31 / 78 (40%) 90 / 105 (86%) Total MUs 617 (69) 614 (72) 606 (36) 0.057 Physician Phys1 3 / 34 (8.8%) 1 / 78 (1.3%) 6 / 105 (5.7%) Phys2 17 / 34 (50%) 51 / 78 (65%) 34 / 105 (32%) Phys3 2 / 34 (5.9%) 2 / 78 (2.6%) 8 / 105 (7.6%) Phys4 3 / 34 (8.8%) 0 / 78 (0%) 2 / 105 (1.9%) Phys5 3 / 34 (8.8%) 1 / 78 (1.3%) 11 / 105 (10%) Phys6 1 / 34 (2.9%) 5 / 78 (6.4%) 5 / 105 (4.8%) Phys7 5 / 34 (15%) 18 / 78 (23%) 39 / 105 (37%) Dosimetrist 0.075 Dos1 14 / 34 (41%) 42 / 78 (54%) 66 / 105 (63%) Dos2 20 / 34 (59%) 36 / 78 (46%) 39 / 105 (37%) TPS Version 0.2 11.0.47 8 / 34 (24%) 22 / 78 (28%) 39 / 105 (37%) 15.1.52 26 / 34 (76%) 56 / 78 (72%) 66 / 105 (63%) Conformation Number 0.87 (0.04) 0.87 (0.05) 0.87 (0.06) 0.7 PTV 72 Gy 119 (72) 102 (42) 93 (37) 0.048 Bladder V65 8 (10) 8 (9) 8 (9) 0.6 Bladder V70 6.4 (7.8) 5.4 (6.9) 5.0 (7.2) 0.4 Rectum V50 21 (10) 22 (10) 24 (8) 0.005 Rectum V60 9.2 (5.7) 9.1 (5.9) 10.4 (5.1) 0.3 Rectum V70 2.90 (2.79) 2.22 (3.41) 2.92 (2.84) 0.062 PO: Prostate Only, PSV: Prostate plus Seminal Vesicles, WPRT: Whole Pelvis Radiation Therapy 1 n / N (%); Median (IQR) 2 Pearson's Chi-squared test; Kruskal-Wallis rank sum test; chi-square test of independence 5.2 Principal Component Analysis Principal component analysis is a widely used dimensionality reduction technique. Given a set of linearly correlated variables, PCA constructs new variables, called principal components, that are linear combinations of the original ones. The rationale is to reduce a larger set of variables into a smaller one, which accounts for most of the original datasets variance. Before applying PCA, we verified whether our data fulfilled some basic assumptions. 5.2.1 Correlation matrix Table 5.1 shows the correlation matrix between different complexity indices, calculated with the Pearson method. Except for the CoA metric, all others had at least one such correlation that exceeded the above threshold. Therefore, CoA was excluded from further analysis. Table 5.1: Correlation matrix of complexity metrics calculated with Pearson method. The requirement for including a variable into PCA was to have at least one correlation with coefficient \\(r \\ge 0.3\\). All variables except for fulfilled the inclusion criterion. coa em esf lt ltmcsv mcs mcsv mfa pi sas coa 1.00 0.12 0.11 0.03 0.01 -0.01 -0.01 0.10 0.12 -0.17 em 0.12 1.00 -0.46 0.28 0.03 -0.32 -0.32 -0.52 -0.41 0.37 esf 0.11 -0.46 1.00 -0.94 -0.74 -0.19 -0.19 0.98 0.99 -0.58 lt 0.03 0.28 -0.94 1.00 0.85 0.31 0.31 -0.89 -0.94 0.52 ltmcsv 0.01 0.03 -0.74 0.85 1.00 0.76 0.76 -0.64 -0.73 0.13 mcs -0.01 -0.32 -0.19 0.31 0.76 1.00 1.00 -0.04 -0.16 -0.41 mcsv -0.01 -0.32 -0.19 0.31 0.76 1.00 1.00 -0.04 -0.16 -0.41 mfa 0.10 -0.52 0.98 -0.89 -0.64 -0.04 -0.04 1.00 0.99 -0.64 pi 0.12 -0.41 0.99 -0.94 -0.73 -0.16 -0.16 0.99 1.00 -0.61 sas -0.17 0.37 -0.58 0.52 0.13 -0.41 -0.41 -0.64 -0.61 1.00 The reason why CoA lacks a significant correlation with the rest of the indices is obviated by looking at the plot of CoA vs. LT in Fig. 5.2. By breaking up the data based on the VMAT plans field size and the number of arcs, two distinct clusters of VMAT plans are emerging. The first one consists mostly of WPRT plans with two arcs, for which the two indices show a strong linear correlation. The second cluster is characterized by small CoA values and large LT, consisting of mostly small field plans delivered via one arc. Similar results may be obtained by comparing CoA vs. PI, EM, and the rest of the indices. Figure 5.2: CoA vs. LT, broken down by field size and number of beams (arcs) per plan. The data form two clusters, depending on the number of arcs. Fig. 5.3 shows the correlations of Table 5.1 visualized in a matrix plot. The squares color is proportional to the Pearson correlation coefficient of the respective pair of CMs. The matrix is symmetric since correlation is commutative, i.e. \\(Corr(x,y) = Corr(y,x)\\). Figure 5.3: Correlation plot of complexity metrics. The threshold for including a variable into PCA was \\(r \\ge 0.3\\). 5.2.2 Sampling adequacy The Kaiser-Meyer-Olkin Measure of Sampling Adequacy indicates the proportion of variance in a dataset that could be caused by underlying factors. High values of KMO (close to 1.0) generally indicate that an explanatory factor analysis may be useful. The MSA values of each individual complexity metric in our dataset were the following: Historically, Kaiser and Rice,43,47 suggested as a rule of thumb that MSA values less than 0.5 are unacceptable. The KMO value of EM was remarkably low, and removing it from the set of variables increased the overall KMO to 0.76. However, given the edge metrics importance, we opted to include it in the PCA, resulting in an overall KMO value of 0.66. However, Bartletts test of sphericity, with edge metric included, produced strong evidence in favor of the existence of adequate correlations in our dataset \\((p &lt; 0.0001)\\). Considering the correlation matrix, the overall KMO criterion, and Bartletts test of sphericity, we concluded that principal component analysis was justified. Figure 5.4: Loading plots of the principal component analysis. Left: Original dataset. Right: Transformed dataset so that LT, MCS, and MCSV increase in the same direction as the rest of the complexity indices, i.e., higher index value corresponds to higher plan complexity. 5.2.3 Loading plot Fig. 5.4 shows two loading plots7 of the principal components analysis. In the left figure, we observe that the various complexity metrics fall into three coarse groups. The first group consists of ESF, MFA, and PI. The second group consists of EM and SAS. The last group loosely consists of MCS and MCSV, along with LT and LTMCSV. However, we keep in mind that LT, MCS, MCSV increase as plan complexity decreases. If we flip their sign for the sole purpose of comparison so that they increase in the same direction as the rest of the complexity indices, i.e., higher index value corresponds to higher plan complexity, we get the figure at the right. Now it becomes evident that LT, too, is positively correlated with ESF, MFA, and PI. In Fig. 5.5, the smooth density histograms of the first cluster are shown, broken down by the number of arcs. All these indices exhibit a bimodal distribution that heavily depends on the number of arcs. However, in our opinion, an ideal complexity metric should be arc agnostic in the sense that its value should not be influenced much by the number of arcs consisting a plan. This fact, along with some other arguments that we discuss later in the linear regression analysis section, contributed to our decision to choose LTMCSV for further analysis. Figure 5.5: Smooth density histograms for ESF, LT, MFA, and PI. The histograms follow a bimodal distribution driven by the number of arcs. Returning to the PCA loading plot, we also notice that the edge metric contributes less to the first two principal components compared to the rest of the indices. This further supports the hypothesis that EM measures some fundamentally different complexity property of the plans, as was implied by the particularly low value of KMO for EM. Next, we will describe the extraction of principal components, and there we will see how EM requires three PCs for its variance to be explained. 5.2.4 Principal components extraction 5.2.4.1 Eigenvalue criterion The eigenvalue-one criterion48 is one of the most popular methods for deciding how many components to retain in the principal component analysis. An eigenvalue less than one indicates that the component explains less variance than a variable would and, therefore, should be omitted. The eigenvalue-one criterion implies that in our case, the first two principal components should be extracted. 5.2.4.2 Scree plot In Fig. 5.6 the explained variance is plotted against the number of principal components. The inflection point of the scree plot is realized at \\(N=3\\). The cumulative explained variance of the data, with 3 PCs, is 96.2%. Figure 5.6: Scree plot of principal components. The inflection point is located at N=3 dimensions. With the use of 3 principal components the total variance of the data that can be accounted for is 96.2%. 5.2.5 Communalities Communality is the proportion of each variables variance accounted for by the principal components. For the \\(i_\\text{th}\\) variable, its communality is computed by taking the sum of the squared loadings up to \\(m\\) principal components, i.e. \\(\\hat{h}_i^2=\\sum_{j=1}^m \\hat{\\varphi}_{ij}^2\\). In the following table we list the communalities of the various complexity indices, by keeping 2 principal components: And, by keeping 3 principal components: Yet again, the edge metric manifests an outlier behavior. Although most of the indices variances are explained by the first two principal components, only 48% of EMs is accounted for. Instead, to explain the majority of EMs variance, three PCs are required. SAS shows a similar behavior, albeit to a lesser degree, with 2 PCs explaining 70%, and 3 PCs 86% of its variance. 5.3 Mutual Information Analysis Figure 5.7: Differential entropy (in nats) of complexity metrics. We calculated the differential entropy for all metrics (Fig. 5.7). For a continuous distribution with known mean, \\(\\mu\\), and known variance, \\(\\sigma^2\\), it can be proven that the maximum entropy distribution is the normal distribution.49 The value of the maximum entropy is \\(\\frac{1}{2} \\left(1 + \\ln\\left(2 \\pi \\sigma^2\\right)\\right)\\). Assuming standardized data, as in our case, i.e. \\(\\sigma = 1\\), it is \\(h(X) \\simeq 1.42\\), however in Fig. 5.7, there are variables exceeding this theoretical upper limit. We hypothesize that this is an issue of numerical integration. The important to note, though, is the relative informational content between different complexity metrics. For instance, ESF, MFA, and PI bear a particularly low one, relative to EM, LTMCSV, or SAS. We calculated the mutual information for every pair of complexity metrics and sorted the probability density distributions by their MI value. Unlike the correlation coefficient, mutual information is more general and captures how much the pairs joint distribution differs. In Fig. 5.8, we compare the MI of two pairs. The first pair is ESF and MFA with high MI, as opposed to the second pair of LT and MCSV, which has low MI. The particularly low mutual information of LT and MCSV provides supporting evidence in favor of the construction of the multiplicative combination, LTMCSV. In Fig. 5.9 we list the joint probability density distributions for all the pairs, sorted by ascending MI. This ranking might prove very useful in the context of creating a new complexity metric. For instance, some promising pairs with high individual entropy, low mutual information, and a strong effect on both first two principal components include EM/SAS, LTMCSV/SAS, LT/MCSV, and EM/SAS, to name a few. Figure 5.8: Contour plots of the joint distributions of two complexity metric pairs, along with their MI. The left pair (ESF, MFA) has almost 12 times higher MI compared to the right pair (LT, MCSV). The construction of a multiplicative combination of two complexity metrics should take into account their MI. Figure 5.9: Contour plots of joint distributions of pairwise complexity metrics, sorted by ascending mutual information. 5.4 Linear Regression Analysis The choice of a complexity metric that correlates well with one departments quality assurance and verification indices has been an elusive goal. Masi et al. 31 found that LT, MCSV, and LTMCS were significantly correlated with VMAT dosimetric accuracy as expressed in gamma passing rates. In our PC analysis, LTMCSV was found to affect both the first and second principal components, presumably because it contains multidimensional information regarding a plans complexity. The pair LT and MCSV had low MI compared to other pairs, implying little redundance. Also, LTMCSV showed a distinct separation between plans with different field sizes, as we would expect for a well-founded complexity metric. Last, as we have already commented, it is arcs agnostic, in the sense that it is not heavily influenced by the number of arcs. For the reasons above, we chose LTMCSV as the complexity metric against which we developed our predictive models. 5.4.1 Multicollinearity detection In the following scatter plot matrix of Fig. 5.10, we see that there are some variables in our dataset that are highly linearly correlated (\\(r &gt; 0.8\\)), and a couple of others that are moderately correlated. Figure 5.10: Matrix scatter plot of the candidate predictor variables in the initial dataset, along with the respective Pearsons correlation coefficients. In the same context, we considered the variance inflation factors. VIFs show how much the variance of the estimated coefficients are inflated compared to the variance when the independent variables are not correlated. Usually, a value less than 4 indicates no multicollinearity. In Table 5.2, we notice that bladder_v65, bladder_v70 have an exceptionally high VIF value. Also rectum_60 and rectum_v70 have a \\(\\text{VIF}&gt;4\\). Based on the VIF values and the previous correlation plots, we chose to drop bladder_v70 and rectum_v70. These variables frequently assume the value of zero, and therefore we hypothesize that they contain less information than bladder_v65 and rectum_v60, respectively. After removing the collinear variables, we re-ran the VIF analysis to check whether the new VIF values have settled below 4. In Table 5.3, we confirm that after removing the two collinear variables, the rest of the variables, although they exhibit some variance inflation, this is below the cutoff of 4. Table 5.2: Table with Variance Inflation Factors in the initial dataset. Est. 2.5% 97.5% t val. p VIF (Intercept) 0.30 0.25 0.36 11.01 0.00 NA total_mus 0.00 0.00 0.00 -1.64 0.10 1.35 arcs2 -0.02 -0.03 -0.01 -4.85 0.00 2.05 field_sizePSV -0.03 -0.03 -0.02 -5.39 0.00 2.31 field_sizeWPRT -0.05 -0.06 -0.04 -9.48 0.00 2.31 physicianPhys2 -0.02 -0.03 0.00 -2.14 0.03 2.61 physicianPhys3 0.00 -0.02 0.02 -0.14 0.89 2.61 physicianPhys4 -0.01 -0.04 0.01 -0.91 0.36 2.61 physicianPhys5 -0.02 -0.03 0.00 -1.69 0.09 2.61 physicianPhys6 0.00 -0.01 0.02 0.53 0.60 2.61 physicianPhys7 -0.02 -0.03 0.00 -2.27 0.02 2.61 dosimetristDos2 0.01 0.00 0.01 1.65 0.10 1.40 tps_version15.1.52 -0.04 -0.05 -0.03 -10.36 0.00 1.74 cn -0.02 -0.05 0.00 -2.08 0.04 1.15 ptv_72 0.00 0.00 0.00 2.46 0.01 1.41 bladder_v65 0.00 0.00 0.00 -0.52 0.60 51.07 bladder_v70 0.00 0.00 0.00 0.76 0.45 52.28 rectum_v50 0.00 0.00 0.00 -4.28 0.00 3.66 rectum_v60 0.00 0.00 0.00 1.56 0.12 7.15 rectum_v70 0.00 0.00 0.00 0.15 0.88 4.39 Table 5.3: Table with Variance Inflation Factors for the remaining variables in the dataset, after removing bladder V70 and rectum V70. Est. 2.5% 97.5% t val. p VIF (Intercept) 0.30 0.25 0.36 11.04 0.00 NA total_mus 0.00 0.00 0.00 -1.64 0.10 1.35 arcs2 -0.02 -0.03 -0.01 -4.98 0.00 1.94 field_sizePSV -0.03 -0.04 -0.02 -5.61 0.00 2.09 field_sizeWPRT -0.05 -0.06 -0.04 -10.07 0.00 2.09 physicianPhys2 -0.02 -0.03 0.00 -2.06 0.04 2.34 physicianPhys3 0.00 -0.02 0.02 -0.07 0.95 2.34 physicianPhys4 -0.01 -0.03 0.01 -0.80 0.43 2.34 physicianPhys5 -0.01 -0.03 0.00 -1.61 0.11 2.34 physicianPhys6 0.01 -0.01 0.02 0.56 0.58 2.34 physicianPhys7 -0.02 -0.03 0.00 -2.23 0.03 2.34 dosimetristDos2 0.01 0.00 0.01 1.60 0.11 1.37 tps_version15.1.52 -0.04 -0.05 -0.03 -10.54 0.00 1.69 cn -0.02 -0.05 0.00 -2.11 0.04 1.15 ptv_72 0.00 0.00 0.00 2.35 0.02 1.37 bladder_v65 0.00 0.00 0.00 1.54 0.13 1.27 rectum_v50 0.00 0.00 0.00 -4.51 0.00 3.42 rectum_v60 0.00 0.00 0.00 2.56 0.01 3.06 5.4.2 Univariate analysis After the removal of the collinear variables, we performed a univariate analysis on the rest of the covariates to decide which one of them would be included in the multivariate regression analysis. Any variable with p&lt;0.2 in the univariate analysis was selected for inclusion in multivariate analysis. The results are summarized in Table 8: Predictor variables, along with their p-value in the univariate analysis, and whether they were included in the multivariate model. Variable Name p-value Inclusion in multivariate model total_mus 0.852 No arcs &lt;0.001 Yes field_size &lt;0.001 Yes physician 0.546 No dosimetrist 0.003 Yes tps_version &lt;0.001 Yes cn 0.616 No ptv_72 0.003 Yes bladder_v65 0.110 Yes rectum_v50 0.031 Yes rectum_v60 0.818 No Fig. 5.11 shows the distribution of LTMCSV against every categorical variable that emerged as statistically significant in the univariate analysis. The difference in complexity is more prominent among plans of different field sizes. An expected result since WPRT contains 3 prescription levels (72 Gy, 57.6 Gy, and 51.2 Gy) and exposes OARs to significantly higher radiation doses, rather than PSV or PO. The differences are less noticeable, albeit present, in the number of arcs and TPS version and even less for the dosimetrist. Similarly, Fig. 5.12 presents plots of LTMCSV vs. the statistically significant continuous variables in the dataset. Regarding dose to the rectum, it appears that as \\(V_{50}\\) increases, so does the complexity. Presumably, the presence of high doses to the rectum during the optimization process drives the dosimetrist to assign a high weight factor in the \\(V_{50}\\) plan objective. This, in turn, leads to an increase in the plans modulation degree and, therefore, complexity. Figure 5.11: Categorical variables that were determined as significant during the univariate analysis. The lower the LTMCSV, the more complex the plan. Figure 5.12: Continuous variables that were determined as significant during the univariate analysis. The lower the LTMCSV, the more complex the plan. 5.4.3 Multivariate analysis 5.4.3.1 Reduced model We ran a multivariate linear model with the seven predictor variables that we selected during the univariate analysis. All variables, except for bladders \\(V_{65}\\), were statistically significant plan complexity predictors. These results are plausible since using two arcs is more common in WPRT plans, which, in turn, are inherently more complicated due to their intricate geometric relations of large treatment volumes and surrounding organs at risk. The treatment planning version effect was also expected because optimization algorithms changed from version 11 to version 15. Finally, PTV 72 Gy and rectums \\(V_{50}\\) are also plausible contributors to complexity as their overlap imposes additional constraints on the plan. The difference between dosimetrists, although statistically significant, is of small magnitude, which can be explained by factoring in the common TPS, the shared plan templates, and cross-pollination of knowledge in our department. 5.4.3.2 Full model We also ran a full model with all the predictor variables included. In Table ??, both the reduced and the full model are summarized. All variables, except for dosimetrist, retained their statistical significance in the multivariate model. Additionally, there was a statistically significant difference in the physician that contoured the plan, although marginal. Last, both the conformation number and rectums \\(V_{60}\\) dose were significant. A higher conformation number implies greater modulation of dose intensity. By the same token, higher rectums \\(V_{60}\\) doses imply less modulation. Since the two models were nested, we used Rs anova() function to compare them. The residual sum of squares (RSS) value of the full model (RSS=0.088) was smaller than the one of the reduced model (RSS=0.104). The partial F-test, which compares the RSS of the two models to see if the change in RSS due to the omission of a term is significant, reported p&lt;0.001. Last, the Akaike Information Criterion of the full model (AIC=-1041) was lower in comparison to the reduced model (AIC=-1023). Therefore, we concluded that the full models additional variables provided statistically significant extra information in predicting a plans complexity when the other explanatory variables have already been considered. In Fig. 5.13, we show some diagnostic plots for evaluating our models validity. In the Residuals vs. Fitted plot, the red line is approximately horizontal, lacking any distinct patterns. This is an indication of a linear association. In the Normal Q-Q plot, the standardized residuals points follow the straight line, representing the perfect percentile line, implying that residuals are normally distributed. In Scale-Location, an approximately horizontal line with equally spread points is a good indication of homoscedasticity. Finally, the Residuals vs. Leverage plot is used to detect influential observations. All things considered, there were no major violations of the underlying assumptions for the linear model (linearity of relationship, normality in the residuals, and equality of variances.) Figure 5.13: Diagnostic plots of the full linear model. In Fig. 5.14 we plotted the real complexity vs. the predicted complexity for the two models (reduced and full). We notice how the two models perform reasonably well. The full models adjusted \\(R^2\\) was 0.671 and reduced models adjusted \\(R^2\\) was 0.627. Figure 5.14: Real complexity vs. predicted complexity for the reduced and full model. The dotted line represents perfect predictions. 5.4.3.3 Backward and forward search We used backward and forward selection to pick the best predictor variables. Although this is usually not the case, both backward and forward selection methods converged on the same set of predictor variables, which included all covariates. 5.5 Logistic Regression analysis Instead of predicting the exact value of complexity, we considered grouping VMAT plans into high complexity and low complexity plans. Ideally, the cutoff value should be chosen based on an analysis of complexity vs. deliverability. However, since this was beyond the thesiss scope, we arbitrarily defined as high complexity plans whose complexity exceeded the datasets median value (Fig. 5.15). We keep in mind that LTMCSV increases as plan complexity decreases. Therefore, plans with an LTMCSV value larger than the datasets median value were designated as low complexity plans. Figure 5.15: Histogram distribution of complexity (LTMCSV). The vertical red line represents the median complexity, which was used as a cut-off value separating high-complexity plans from low-complexity ones. LTMCSV increases as complexity decreases, therefore the area to the right of the red line corresponds to low complexity plans. The following plots in Fig. 5.16 were generated with glmnet during the process of solving the logistic regression problem with LASSO regularization. The first figure shows binomial deviance as a function of the regularization parameter \\(\\lambda\\). The optimal value of \\(\\lambda\\) was chosen that minimized the 5-fold cross-validated binomial deviance (\\(\\lambda_{\\text{opt}}=0.0251\\)). The second plot shows the models coefficients values for increasing values of the regularization parameter \\(\\lambda\\). It is evident that as \\(\\lambda\\) grows, LASSO is invoking sparsity to the model by forcing some of the coefficients to become zero, as opposed to ridge regression (Table 5.4). In the limit where \\(\\lambda\\) grows large, all the coefficients of the model become zero. This is a well-known property of LASSO, and it is used as a feature selection process. The last figure shows percent of deviance vs. coefficients. Toward the end of the path, the explained deviance values are not changing much, but the coefficients start to blow up. This information can be used to identify the parts of the fit that matter. Figure 5.16: Binomial deviance and model coefficients along the regularization path. LASSO regression invokes sparsity by driving certain coefficients to zero, as the value of \\(\\lambda\\) increases, performing essentially feature selection. After having trained our logistic regression model on the training set, we evaluated its performance with confusion matrices (Fig. 5.17). The value of 1 corresponds to the class label low complexity plan, and the value of 0 corresponds to the class label high complexity plan. The accuracy of the model was 87.25%, and the AUC 0.906 (Fig. 5.18). Of particular interest is the false positive rate (FP). I.e., plans classified as low complexity, whereas their correct class was high complexity. The FP rate in the train set was 14.6%, and in the test set, 14.8%. Since FP is of greater importance than FN, one could tune the model to exhibit larger specificity at the cost of lower sensitivity. Figure 5.17: Confusion matrices of the best LASSO logistic regression model for the training set (left) and for the test set (right). Class 1 corresponds to low-complexity plans, and Class 0 to high-complexity plans. Figure 5.18: ROC evaluated in the training set (left) and in the test set (right). The dashed lines represent a random classifier. AUC: Area Under Curve, TPR: True Positive Rate, FPR: False Positive Rate. Table 5.4: Summary table of best LASSO model. The parameter lambda corresponds to the optimal \\(\\lambda\\) that was calculated during the k-fold cross-validation. term step estimate lambda dev.ratio (Intercept) 1 2.779 0.025 0.446 arcs2 1 -2.181 0.025 0.446 field_sizeWPRT 1 -1.490 0.025 0.446 physicianPhys3 1 1.243 0.025 0.446 physicianPhys4 1 -2.123 0.025 0.446 physicianPhys6 1 1.483 0.025 0.446 physicianPhys7 1 -0.198 0.025 0.446 dosimetristDos2 1 0.073 0.025 0.446 tps_version15.1.52 1 -2.067 0.025 0.446 cn 1 -0.584 0.025 0.446 ptv_72 1 0.006 0.025 0.446 bladder_v70 1 0.032 0.025 0.446 rectum_v70 1 0.075 0.025 0.446 51.2 Gy is the elective dose pelvic lymph nodes receive at our department. A loading plot shows how strongly each variable influences a principal component. "],["discussion.html", "6 Discussion 6.1 General 6.2 Complexity metrics 6.3 Complexity prediction 6.4 Plan classification 6.5 Future work", " 6 Discussion 6.1 General VMAT radiotherapy is currently the best treatment delivery method for prostate cancer, but VMAT plans have grown inherently complex, consisting of hundreds of small, irregularly shaped apertures with varying beam intensity. When this complexity increases uncontrollably, it may become problematic in terms of, among others, of actual plan deliverability to the patient. Hence, VMAT plans need to undergo meticulous quality assurance and verification processes before being delivered to the patients.50 These procedures require expensive equipment, extensive expertise by the medical physicist, and reduce the availability of the linear accelerator. Therefore, there is a genuine need to streamline the verification process. A predictive QA system based on machine-learning would benefit both radiotherapy departments personnel and cancer patients. The former would be disengaged from the time-consuming verification processes and direct resources to other aspects of radiotherapys workflow, such as QA of the linear accelerator or treatment planning. Likewise, patients would benefit from decreased waiting times. Both of these could, in principle, increase the RT departments patient throughput and help replace conventional 3D conformal radiotherapy with state-of-the-art VMAT treatments. In the past, there have been efforts to correlate plan complexity with quality assurance indices, such as gamma passing rates. The rationale for this proposition is the following. If we managed to correlate complexity with QA outcomes, we could conceivably bypass the physical verification process merely by calculating the plans complexity. However, except for some sporadic positive results,26,27,31 the correlation of these two has remained an elusive goal.51 Another path has been the development of artificial-intelligence (AI) models that predict the verification indices (Fig. 6.1). The proof of this concept was demonstrated in 2018 by Tomori et al.52 The researchers trained a 15 layer convolutional neural network to learn the sagittal planar dose distributions from a QA phantom and produce the 2%/2 mm, 3%/2 mm, 2%/3 mm, and 3%/3 mm gamma passing rates. The networks input also included the planning target volume, the volume of the rectum, and their overlap region, along with the number of monitor units for each IMRT field. Since then, many other similar papers have been published.5357 Figure 6.1: Deep neural network architecture of a predictive QA system. Complexity metrics could be used as potential input features in an AI model. This thesis examined several complexity metrics and attempted to describe a framework for selecting the best index out of the many that exist. We also constructed predictive models of complexity by considering clinical and dosimetric plan features. Our approach compares favorably to sophisticated machine learning models (e.g., neural networks) in that linear and logistic regression models are more interpretable. This is a well-known limitation of machine learning and an area of active research on how to improve them.58 Nevertheless, complexity metrics could be used in conjunction with neural networks (Fig. 6.1) by acting as potential input features.57 6.2 Complexity metrics In the present study, we dealt with prostate cancer patients undergoing radical treatment with VMAT. After extracting the plans from the oncology information system, we calculated ten complexity indices described in the literature. We then analyzed them with principal component analysis. PCA is a widely used dimensionality reduction technique that exploits linear correlations in a dataset. Through PCA, we can replace the existing variables with new ones, the so-called principal components, yet preserve the qualitative features of the original data.59 Using three principal components, we accounted for 96.2% of the datasets variance, effectively reducing its dimensionality from 10 to 3. We also generated a loading plot and recognized the clustering of complexity metrics into three distinct groups. The first cluster consists of ESF, MFA, and PI. The second consists of EM and SAS, and the last one, the less well-defined, of MCS and MCSV, along with LT and LTMCSV. However, if we consider that LT increases as plan complexity decreases, we conclude that LT too clusters along with ESF, MFA, and PI. These findings suggest that there exists a substantial informational overlap among the various complexity metrics. Given that many of them measure similar aspects of a plans aspect, this outcome was expected. For instance, ESF, MFA, and PI are strongly influenced by the fields open area. The importance of principal component analysis is bifold. First, it provides insights regarding the interconnectedness of different complexity metrics. In the literature, there are more than forty complexity metrics.25 Based on our PCA and MIA analyses, we hypothesize that there is considerable overlap among them. Second, it opens up new routes to pursue an index that would show a robust correlation with QA metrics. As we have mentioned, the connection between complexity and QA metrics has not been fully realized. To overcome this, we could use a PCA informed linear combination of existing complexity metrics to construct a novel metric. This new index would, in principle, combine the unique informational content of all the individual indices. To the best of our knowledge, there have been only a few published papers utilizing PCA to study VMAT plans complexity.60 In a similar context, we calculated the entropy of all metrics and mutual information for every complexity metric pair. For \\(N=10\\) metrics, there were \\(N(N-1)/2=45\\) pairs. MI is a more robust measure, compared to the correlation coefficient, and captures how much a complexitys pairs joint distribution differs from the product of each metrics marginal distributions. In other words, it quantifies the informational content one obtains about a metric through the observation of another one. This information-theoretic perspective could enrich our understanding of VMAT plans complexity. It could also help us navigate the landscape of various complexity indices by combining those with the lowest possible mutual information. This is the case of LT and MCSV, whose MI was very low compared to other pairs (Fig. 5.8). Therefore, the construction of the multiplicative combination, LTMCSV, was sound. Likewise, the choice of LTMCSV as the response variable for the development of predictive models. 6.3 Complexity prediction 6.3.1 The choice of LTMCSV The choice of a complexity metric that correlates well with one departments physical verification outcomes has been the holy grail of QA. Masi et al. found that LT, MCSV, and LTMCS were all significantly correlated with VMAT dosimetric accuracy expressed in gamma passing rates. In our PC analysis, LTMCSV was found to exert a strong effect on both the first and second principal components. We presume that this is because it carries detailed multidimensional information regarding a plans complexity. Also, the pair LT, MCSV had low mutual information, justifying the use of their multiplicative combination. Moreover, LTMCSV showed a clear separation between plans with different field sizes, which we would expect for a reliable complexity metric. The last argument in favor of LTMCSV is that it was arcs agnostic, in the sense that the number of arcs did not exert much influence on it. For these reasons, we chose to adopt LTMCSV as our response variable while developing the predictive models. However, we keep in mind that every TPS assigns different priorities when modulating a VMAT plan.24 Therefore, for Phillips Pinnacle or Elekta Monaco plans, perhaps \\(\\text{MI}_{t}\\) would be better suited since it accounts for variations in gantry speed, dose rate, and acceleration of the MLC.32 6.3.2 Linear regression model In the second part of the thesis, we used linear regression analysis to predict the VMAT plans complexity from basic clinical and dosimetric features. First, we examined the candidate predictor variables for linear correlations. The latter may cause instability in the estimated coefficients, inflate standard errors and confidence intervals for the models parameters, hurting the models interpretability. For these reasons, we performed multicollinearity detection through scatter plot visualization and variance inflation factors. In the end, we identified bladders \\(V_{70}\\) and rectums \\(V_{70}\\) as the most significant offenders and omitted them from the list of covariates. Subsequently, we evaluated various linear regression models constructed through univariate and multivariate analysis, backward and forward selection methods. The evaluation criteria included RSS, adjusted R-squared, and the AIC criterion. The best model turned out to be the one containing all candidate predictors with an adjusted \\(R^2=0.671\\). The variables that were correlated with increased complexity (i.e., a lower value of LTMCSV) were plans with two arcs (p&lt;0.001), increasing field size (p&lt;0.001), the 15.1.52 treatment planning version (p&lt;0.001), the high dose PTV of 72 Gy (p=0.02), rectums \\(V_{50}\\) (p&lt;0.001), physicians 2 and 7 (p=0.041 and p=0.027, respectively), and high conformation number (p=0.036). The variable that correlated with decreased complexity (i.e., a higher value of LTMCSV) was rectums \\(V_{60}\\) (p=0.011). It is noteworthy that the linear model overestimated complexity (Fig. 5.14). In a production setup, it is preferable to characterize simple plans as complex and have them physically verified, rather than the opposite, i.e., erroneously designate a complex plan as simple and bypass the physical QA procedures. Therefore, the model erred on the right side. 6.4 Plan classification Following the development of a linear regression model, we dichotomized VMAT plans into high complexity and low complexity ones. Ideally, the threshold choice should be derived from an analysis of complexity vs. quality assurance metrics, such as the gamma passing rates. I.e., plans designated as low complexity, under the chosen threshold, should correlate with higher scores in verification procedures. However, this analysis was beyond the scope of this thesis. Instead, we arbitrarily defined high complexity plans whose complexity exceeded the datasets median value. After labeling every plan based on its complexity, we split the dataset into a training set (75%) and a test set (25%). We then performed a 5-fold cross-validated logistic regression analysis with LASSO regularization on the former. LASSO is known to invoke sparsity by driving some of the models coefficients to zero, which results in feature selection. After determining the optimal cross-validated value of the regularization parameter \\(\\lambda\\) in the training set, we evaluated the best LASSO model to the test set. The accuracy of the model in the test set was 87.25%, and the AUC 0.906. The false-positive rate8 in the test set was 14.8%. Considering the importance of missing a complex plan, one could tune the model to exhibit larger specificity (less FP rate) at the expense of lower sensitivity (higher FN rate). 6.5 Future work One of the most established QA tools for planar dose verification in IMRT/VMAT is the gamma index analysis, which considers the dose difference (DD) and distance to agreement (DTA) between two distributions.61 Concretely, for each point \\(\\mathbf{r}_{m}\\) in the measured distribution, a dimensionless distance metric is calculated via the formula: \\[\\begin{align*} \\Gamma(\\mathbf{r}_m, \\mathbf{r}_c) = \\sqrt{\\frac{r^2(\\mathbf{r}_m, \\mathbf{r}_c)}{\\Delta d_M^2} + \\frac{\\delta^2(\\mathbf{r}_m, \\mathbf{r}_c)}{\\Delta D_M^2}} \\end{align*}\\] Where \\(r(\\mathbf{r}_m, \\mathbf{r}_c) = \\left|\\mathbf{r}_c  \\mathbf{r}_m\\right|\\) is the distance to agreement and \\(\\delta(\\mathbf{r}_m, \\mathbf{r}_c) = D_c (\\mathbf{r}_c)  D_m (\\mathbf{r}_m)\\) is the dose difference. Finally, \\(\\gamma\\) at point \\(\\mathbf{r}_m\\) is defined as the minimum value: \\[\\begin{align*} \\gamma(\\mathbf{r}_m) = \\text{min}\\{\\Gamma(\\mathbf{r}_m , \\mathbf{r}_c)\\} \\forall \\mathbf{r}_c \\end{align*}\\] The total percentage of \\(\\mathbf{r}_m\\) points that have achieved \\(\\gamma(\\mathbf{r}_m) \\le 1\\), for a given DD/DTA criteria, is calculated, and a pass/fail threshold is set to determine the verification outcome (pass or fail). Despite its limitations, this analysis is widely adopted in clinical practice and endorsed by the American Association of Physicists in Medicine in Task Group 119, where commissioning tests are described in terms of \\(\\gamma\\) index acceptability of 3% dose difference and 3-mm DTA.62 In the future, we plan to explore whether LTMCSV and novel complexity indices, derived by considering the results of PCA and MIA analyses, could correlate with gamma passing rates in our department. The probability of characterizing a high-complexity plan as low-complexity. "],["summary.html", "7 Summary", " 7 Summary During past years, the introduction of VMAT and IGRT has improved radiotherapys therapeutic ratio in prostate cancer. Tight dose conformance and normal tissue sparing have resulted in plans of increased complexity. Such highly sophisticated plans may become problematic if their complexity exceeds some critical threshold. There is an abundance of aperture-based complexity metrics for VMAT plans in the literature. PCA and MIA suggest both shared and unique informational content among many complexity metrics. The ability to predict VMAT plan complexity from clinical and dosimetric parameters is feasible in principle. Both linear and logistic regression models showed promising outcomes. However, more work is needed to validate their results in larger datasets before being used in production. "],["references.html", "References", " References "]]
